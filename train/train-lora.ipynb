{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HDJUrFOrYEfL",
    "outputId": "0a6c954f-1d15-479a-e068-f26319d422e0",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: datasets in /home/phil/work/mb/easybits/factory/.venv/lib/python3.11/site-packages (2.18.0)\n",
      "Requirement already satisfied: accelerate in /home/phil/work/mb/easybits/factory/.venv/lib/python3.11/site-packages (0.29.1)\n",
      "Requirement already satisfied: wandb in /home/phil/work/mb/easybits/factory/.venv/lib/python3.11/site-packages (0.16.6)\n",
      "Requirement already satisfied: peft in /home/phil/work/mb/easybits/factory/.venv/lib/python3.11/site-packages (0.10.0)\n",
      "Requirement already satisfied: filelock in /home/phil/work/mb/easybits/factory/.venv/lib/python3.11/site-packages (from datasets) (3.13.3)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/phil/work/mb/easybits/factory/.venv/lib/python3.11/site-packages (from datasets) (1.26.4)\n",
      "Requirement already satisfied: pyarrow>=12.0.0 in /home/phil/work/mb/easybits/factory/.venv/lib/python3.11/site-packages (from datasets) (15.0.2)\n",
      "Requirement already satisfied: pyarrow-hotfix in /home/phil/work/mb/easybits/factory/.venv/lib/python3.11/site-packages (from datasets) (0.6)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /home/phil/work/mb/easybits/factory/.venv/lib/python3.11/site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pandas in /home/phil/work/mb/easybits/factory/.venv/lib/python3.11/site-packages (from datasets) (2.2.1)\n",
      "Requirement already satisfied: requests>=2.19.0 in /home/phil/work/mb/easybits/factory/.venv/lib/python3.11/site-packages (from datasets) (2.31.0)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in /home/phil/work/mb/easybits/factory/.venv/lib/python3.11/site-packages (from datasets) (4.66.2)\n",
      "Requirement already satisfied: xxhash in /home/phil/work/mb/easybits/factory/.venv/lib/python3.11/site-packages (from datasets) (3.4.1)\n",
      "Requirement already satisfied: multiprocess in /home/phil/work/mb/easybits/factory/.venv/lib/python3.11/site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2024.2.0,>=2023.1.0 in /home/phil/work/mb/easybits/factory/.venv/lib/python3.11/site-packages (from fsspec[http]<=2024.2.0,>=2023.1.0->datasets) (2024.2.0)\n",
      "Requirement already satisfied: aiohttp in /home/phil/work/mb/easybits/factory/.venv/lib/python3.11/site-packages (from datasets) (3.9.3)\n",
      "Requirement already satisfied: huggingface-hub>=0.19.4 in /home/phil/work/mb/easybits/factory/.venv/lib/python3.11/site-packages (from datasets) (0.22.2)\n",
      "Requirement already satisfied: packaging in /home/phil/work/mb/easybits/factory/.venv/lib/python3.11/site-packages (from datasets) (24.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/phil/work/mb/easybits/factory/.venv/lib/python3.11/site-packages (from datasets) (6.0.1)\n",
      "Requirement already satisfied: psutil in /home/phil/work/mb/easybits/factory/.venv/lib/python3.11/site-packages (from accelerate) (5.9.8)\n",
      "Requirement already satisfied: torch>=1.10.0 in /home/phil/work/mb/easybits/factory/.venv/lib/python3.11/site-packages (from accelerate) (2.2.2)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in /home/phil/work/mb/easybits/factory/.venv/lib/python3.11/site-packages (from accelerate) (0.4.2)\n",
      "Requirement already satisfied: Click!=8.0.0,>=7.1 in /home/phil/work/mb/easybits/factory/.venv/lib/python3.11/site-packages (from wandb) (8.1.7)\n",
      "Requirement already satisfied: GitPython!=3.1.29,>=1.0.0 in /home/phil/work/mb/easybits/factory/.venv/lib/python3.11/site-packages (from wandb) (3.1.43)\n",
      "Requirement already satisfied: sentry-sdk>=1.0.0 in /home/phil/work/mb/easybits/factory/.venv/lib/python3.11/site-packages (from wandb) (1.44.1)\n",
      "Requirement already satisfied: docker-pycreds>=0.4.0 in /home/phil/work/mb/easybits/factory/.venv/lib/python3.11/site-packages (from wandb) (0.4.0)\n",
      "Requirement already satisfied: setproctitle in /home/phil/work/mb/easybits/factory/.venv/lib/python3.11/site-packages (from wandb) (1.3.3)\n",
      "Requirement already satisfied: setuptools in /home/phil/work/mb/easybits/factory/.venv/lib/python3.11/site-packages (from wandb) (69.1.1)\n",
      "Requirement already satisfied: appdirs>=1.4.3 in /home/phil/work/mb/easybits/factory/.venv/lib/python3.11/site-packages (from wandb) (1.4.4)\n",
      "Requirement already satisfied: protobuf!=4.21.0,<5,>=3.19.0 in /home/phil/work/mb/easybits/factory/.venv/lib/python3.11/site-packages (from wandb) (4.25.3)\n",
      "Requirement already satisfied: transformers in /home/phil/work/mb/easybits/factory/.venv/lib/python3.11/site-packages (from peft) (4.39.3)\n",
      "Requirement already satisfied: six>=1.4.0 in /home/phil/work/mb/easybits/factory/.venv/lib/python3.11/site-packages (from docker-pycreds>=0.4.0->wandb) (1.16.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /home/phil/work/mb/easybits/factory/.venv/lib/python3.11/site-packages (from aiohttp->datasets) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /home/phil/work/mb/easybits/factory/.venv/lib/python3.11/site-packages (from aiohttp->datasets) (23.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /home/phil/work/mb/easybits/factory/.venv/lib/python3.11/site-packages (from aiohttp->datasets) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /home/phil/work/mb/easybits/factory/.venv/lib/python3.11/site-packages (from aiohttp->datasets) (6.0.5)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /home/phil/work/mb/easybits/factory/.venv/lib/python3.11/site-packages (from aiohttp->datasets) (1.9.4)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in /home/phil/work/mb/easybits/factory/.venv/lib/python3.11/site-packages (from GitPython!=3.1.29,>=1.0.0->wandb) (4.0.11)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/phil/work/mb/easybits/factory/.venv/lib/python3.11/site-packages (from huggingface-hub>=0.19.4->datasets) (4.11.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/phil/work/mb/easybits/factory/.venv/lib/python3.11/site-packages (from requests>=2.19.0->datasets) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/phil/work/mb/easybits/factory/.venv/lib/python3.11/site-packages (from requests>=2.19.0->datasets) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/phil/work/mb/easybits/factory/.venv/lib/python3.11/site-packages (from requests>=2.19.0->datasets) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/phil/work/mb/easybits/factory/.venv/lib/python3.11/site-packages (from requests>=2.19.0->datasets) (2024.2.2)\n",
      "Requirement already satisfied: sympy in /home/phil/work/mb/easybits/factory/.venv/lib/python3.11/site-packages (from torch>=1.10.0->accelerate) (1.12)\n",
      "Requirement already satisfied: networkx in /home/phil/work/mb/easybits/factory/.venv/lib/python3.11/site-packages (from torch>=1.10.0->accelerate) (3.3)\n",
      "Requirement already satisfied: jinja2 in /home/phil/work/mb/easybits/factory/.venv/lib/python3.11/site-packages (from torch>=1.10.0->accelerate) (3.1.3)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /home/phil/work/mb/easybits/factory/.venv/lib/python3.11/site-packages (from torch>=1.10.0->accelerate) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /home/phil/work/mb/easybits/factory/.venv/lib/python3.11/site-packages (from torch>=1.10.0->accelerate) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /home/phil/work/mb/easybits/factory/.venv/lib/python3.11/site-packages (from torch>=1.10.0->accelerate) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /home/phil/work/mb/easybits/factory/.venv/lib/python3.11/site-packages (from torch>=1.10.0->accelerate) (8.9.2.26)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /home/phil/work/mb/easybits/factory/.venv/lib/python3.11/site-packages (from torch>=1.10.0->accelerate) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /home/phil/work/mb/easybits/factory/.venv/lib/python3.11/site-packages (from torch>=1.10.0->accelerate) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /home/phil/work/mb/easybits/factory/.venv/lib/python3.11/site-packages (from torch>=1.10.0->accelerate) (10.3.2.106)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /home/phil/work/mb/easybits/factory/.venv/lib/python3.11/site-packages (from torch>=1.10.0->accelerate) (11.4.5.107)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /home/phil/work/mb/easybits/factory/.venv/lib/python3.11/site-packages (from torch>=1.10.0->accelerate) (12.1.0.106)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.19.3 in /home/phil/work/mb/easybits/factory/.venv/lib/python3.11/site-packages (from torch>=1.10.0->accelerate) (2.19.3)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /home/phil/work/mb/easybits/factory/.venv/lib/python3.11/site-packages (from torch>=1.10.0->accelerate) (12.1.105)\n",
      "Requirement already satisfied: triton==2.2.0 in /home/phil/work/mb/easybits/factory/.venv/lib/python3.11/site-packages (from torch>=1.10.0->accelerate) (2.2.0)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /home/phil/work/mb/easybits/factory/.venv/lib/python3.11/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.10.0->accelerate) (12.4.127)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/phil/work/mb/easybits/factory/.venv/lib/python3.11/site-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/phil/work/mb/easybits/factory/.venv/lib/python3.11/site-packages (from pandas->datasets) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /home/phil/work/mb/easybits/factory/.venv/lib/python3.11/site-packages (from pandas->datasets) (2024.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/phil/work/mb/easybits/factory/.venv/lib/python3.11/site-packages (from transformers->peft) (2023.12.25)\n",
      "Requirement already satisfied: tokenizers<0.19,>=0.14 in /home/phil/work/mb/easybits/factory/.venv/lib/python3.11/site-packages (from transformers->peft) (0.15.2)\n",
      "Requirement already satisfied: smmap<6,>=3.0.1 in /home/phil/work/mb/easybits/factory/.venv/lib/python3.11/site-packages (from gitdb<5,>=4.0.1->GitPython!=3.1.29,>=1.0.0->wandb) (5.0.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/phil/work/mb/easybits/factory/.venv/lib/python3.11/site-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.5)\n",
      "Requirement already satisfied: mpmath>=0.19 in /home/phil/work/mb/easybits/factory/.venv/lib/python3.11/site-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install datasets accelerate wandb peft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PaQ5t8UY1BlX",
    "outputId": "209065ba-7736-4121-8d1a-7de691e165cc"
   },
   "outputs": [],
   "source": [
    "from wandb import login\n",
    "\n",
    "login(\"never\", \"YOUR_WANDB_TOKEN\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hPi2ZNoFh9xN",
    "outputId": "8262ae94-bc06-4616-b2cc-fbab0c116e76",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!huggingface-cli login --token YOUR_HF_TOKEN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HfG_fx-ShLR8",
    "outputId": "9c1b8315-1615-421b-8265-2bc551e9450a",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "fatal: destination path 'diffusers' already exists and is not an empty directory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Obtaining file:///home/phil/work/mb/easybits/factory/train/diffusers\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Checking if build backend supports build_editable: started\n",
      "  Checking if build backend supports build_editable: finished with status 'done'\n",
      "  Getting requirements to build editable: started\n",
      "  Getting requirements to build editable: finished with status 'done'\n",
      "  Installing backend dependencies: started\n",
      "  Installing backend dependencies: finished with status 'done'\n",
      "  Preparing editable metadata (pyproject.toml): started\n",
      "  Preparing editable metadata (pyproject.toml): finished with status 'done'\n",
      "Requirement already satisfied: importlib-metadata in /home/phil/work/mb/easybits/factory/.venv/lib/python3.11/site-packages (from diffusers==0.28.0.dev0) (7.1.0)\n",
      "Requirement already satisfied: filelock in /home/phil/work/mb/easybits/factory/.venv/lib/python3.11/site-packages (from diffusers==0.28.0.dev0) (3.13.3)\n",
      "Requirement already satisfied: huggingface-hub>=0.20.2 in /home/phil/work/mb/easybits/factory/.venv/lib/python3.11/site-packages (from diffusers==0.28.0.dev0) (0.22.2)\n",
      "Requirement already satisfied: numpy in /home/phil/work/mb/easybits/factory/.venv/lib/python3.11/site-packages (from diffusers==0.28.0.dev0) (1.26.4)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/phil/work/mb/easybits/factory/.venv/lib/python3.11/site-packages (from diffusers==0.28.0.dev0) (2023.12.25)\n",
      "Requirement already satisfied: requests in /home/phil/work/mb/easybits/factory/.venv/lib/python3.11/site-packages (from diffusers==0.28.0.dev0) (2.31.0)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in /home/phil/work/mb/easybits/factory/.venv/lib/python3.11/site-packages (from diffusers==0.28.0.dev0) (0.4.2)\n",
      "Requirement already satisfied: Pillow in /home/phil/work/mb/easybits/factory/.venv/lib/python3.11/site-packages (from diffusers==0.28.0.dev0) (10.3.0)\n",
      "Requirement already satisfied: torch>=1.4 in /home/phil/work/mb/easybits/factory/.venv/lib/python3.11/site-packages (from diffusers==0.28.0.dev0) (2.2.2)\n",
      "Requirement already satisfied: accelerate>=0.11.0 in /home/phil/work/mb/easybits/factory/.venv/lib/python3.11/site-packages (from diffusers==0.28.0.dev0) (0.29.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/phil/work/mb/easybits/factory/.venv/lib/python3.11/site-packages (from accelerate>=0.11.0->diffusers==0.28.0.dev0) (24.0)\n",
      "Requirement already satisfied: psutil in /home/phil/work/mb/easybits/factory/.venv/lib/python3.11/site-packages (from accelerate>=0.11.0->diffusers==0.28.0.dev0) (5.9.8)\n",
      "Requirement already satisfied: pyyaml in /home/phil/work/mb/easybits/factory/.venv/lib/python3.11/site-packages (from accelerate>=0.11.0->diffusers==0.28.0.dev0) (6.0.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /home/phil/work/mb/easybits/factory/.venv/lib/python3.11/site-packages (from huggingface-hub>=0.20.2->diffusers==0.28.0.dev0) (2024.2.0)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /home/phil/work/mb/easybits/factory/.venv/lib/python3.11/site-packages (from huggingface-hub>=0.20.2->diffusers==0.28.0.dev0) (4.66.2)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/phil/work/mb/easybits/factory/.venv/lib/python3.11/site-packages (from huggingface-hub>=0.20.2->diffusers==0.28.0.dev0) (4.11.0)\n",
      "Requirement already satisfied: sympy in /home/phil/work/mb/easybits/factory/.venv/lib/python3.11/site-packages (from torch>=1.4->diffusers==0.28.0.dev0) (1.12)\n",
      "Requirement already satisfied: networkx in /home/phil/work/mb/easybits/factory/.venv/lib/python3.11/site-packages (from torch>=1.4->diffusers==0.28.0.dev0) (3.3)\n",
      "Requirement already satisfied: jinja2 in /home/phil/work/mb/easybits/factory/.venv/lib/python3.11/site-packages (from torch>=1.4->diffusers==0.28.0.dev0) (3.1.3)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /home/phil/work/mb/easybits/factory/.venv/lib/python3.11/site-packages (from torch>=1.4->diffusers==0.28.0.dev0) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /home/phil/work/mb/easybits/factory/.venv/lib/python3.11/site-packages (from torch>=1.4->diffusers==0.28.0.dev0) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /home/phil/work/mb/easybits/factory/.venv/lib/python3.11/site-packages (from torch>=1.4->diffusers==0.28.0.dev0) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /home/phil/work/mb/easybits/factory/.venv/lib/python3.11/site-packages (from torch>=1.4->diffusers==0.28.0.dev0) (8.9.2.26)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /home/phil/work/mb/easybits/factory/.venv/lib/python3.11/site-packages (from torch>=1.4->diffusers==0.28.0.dev0) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /home/phil/work/mb/easybits/factory/.venv/lib/python3.11/site-packages (from torch>=1.4->diffusers==0.28.0.dev0) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /home/phil/work/mb/easybits/factory/.venv/lib/python3.11/site-packages (from torch>=1.4->diffusers==0.28.0.dev0) (10.3.2.106)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /home/phil/work/mb/easybits/factory/.venv/lib/python3.11/site-packages (from torch>=1.4->diffusers==0.28.0.dev0) (11.4.5.107)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /home/phil/work/mb/easybits/factory/.venv/lib/python3.11/site-packages (from torch>=1.4->diffusers==0.28.0.dev0) (12.1.0.106)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.19.3 in /home/phil/work/mb/easybits/factory/.venv/lib/python3.11/site-packages (from torch>=1.4->diffusers==0.28.0.dev0) (2.19.3)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /home/phil/work/mb/easybits/factory/.venv/lib/python3.11/site-packages (from torch>=1.4->diffusers==0.28.0.dev0) (12.1.105)\n",
      "Requirement already satisfied: triton==2.2.0 in /home/phil/work/mb/easybits/factory/.venv/lib/python3.11/site-packages (from torch>=1.4->diffusers==0.28.0.dev0) (2.2.0)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /home/phil/work/mb/easybits/factory/.venv/lib/python3.11/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.4->diffusers==0.28.0.dev0) (12.4.127)\n",
      "Requirement already satisfied: zipp>=0.5 in /home/phil/work/mb/easybits/factory/.venv/lib/python3.11/site-packages (from importlib-metadata->diffusers==0.28.0.dev0) (3.18.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/phil/work/mb/easybits/factory/.venv/lib/python3.11/site-packages (from requests->diffusers==0.28.0.dev0) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/phil/work/mb/easybits/factory/.venv/lib/python3.11/site-packages (from requests->diffusers==0.28.0.dev0) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/phil/work/mb/easybits/factory/.venv/lib/python3.11/site-packages (from requests->diffusers==0.28.0.dev0) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/phil/work/mb/easybits/factory/.venv/lib/python3.11/site-packages (from requests->diffusers==0.28.0.dev0) (2024.2.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/phil/work/mb/easybits/factory/.venv/lib/python3.11/site-packages (from jinja2->torch>=1.4->diffusers==0.28.0.dev0) (2.1.5)\n",
      "Requirement already satisfied: mpmath>=0.19 in /home/phil/work/mb/easybits/factory/.venv/lib/python3.11/site-packages (from sympy->torch>=1.4->diffusers==0.28.0.dev0) (1.3.0)\n",
      "Checking if build backend supports build_editable: started\n",
      "Checking if build backend supports build_editable: finished with status 'done'\n",
      "Building wheels for collected packages: diffusers\n",
      "  Building editable for diffusers (pyproject.toml): started\n",
      "  Building editable for diffusers (pyproject.toml): finished with status 'done'\n",
      "  Created wheel for diffusers: filename=diffusers-0.28.0.dev0-0.editable-py3-none-any.whl size=11134 sha256=3eafab6219093182d3a1d3d293fbfbef1d125ab7195b4eb0b0e6395792b2cd74\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-_823bzgw/wheels/3f/e5/54/db303e19a0c08228f2dec1ef63e92be35a149c7e3e06d3e19b\n",
      "Successfully built diffusers\n",
      "Installing collected packages: diffusers\n",
      "  Attempting uninstall: diffusers\n",
      "    Found existing installation: diffusers 0.27.2\n",
      "    Uninstalling diffusers-0.27.2:\n",
      "      Successfully uninstalled diffusers-0.27.2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "factory 0.0.0a0 requires diffusers<0.28.0,>=0.27.2, but you have diffusers 0.28.0.dev0 which is incompatible.\u001b[0m\u001b[31m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully installed diffusers-0.28.0.dev0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "git clone https://github.com/huggingface/diffusers.git\n",
    "cd diffusers\n",
    "pip install -e \".[torch]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration already exists at /home/phil/.cache/huggingface/accelerate/default_config.yaml, will not override. Run `accelerate config` manually or pass a different `save_location`.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from accelerate.utils import write_basic_config\n",
    "\n",
    "write_basic_config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hvHiknuIg3pp",
    "outputId": "b5620a9a-d705-449e-e4be-b92d01f08bcb"
   },
   "outputs": [],
   "source": [
    "!scp diffusers/examples/dreambooth/train_dreambooth_lora_sdxl.py train_dreambooth_lora.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p models/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zKulUFd8fS_T",
    "outputId": "ca869967-2146-43b7-fba0-f2d55294c41d",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "04/07/2024 20:43:16 - INFO - __main__ - Distributed environment: DistributedType.NO\n",
      "Num processes: 1\n",
      "Process index: 0\n",
      "Local process index: 0\n",
      "Device: cuda\n",
      "\n",
      "Mixed precision type: fp16\n",
      "\n",
      "You are using a model of type clip_text_model to instantiate a model of type . This is not supported for all configurations of models and can yield errors.\n",
      "You are using a model of type clip_text_model to instantiate a model of type . This is not supported for all configurations of models and can yield errors.\n",
      "{'thresholding', 'rescale_betas_zero_snr', 'dynamic_thresholding_ratio', 'variance_type', 'clip_sample_range'} was not found in config. Values will be initialized to default values.\n",
      "{'latents_mean', 'latents_std'} was not found in config. Values will be initialized to default values.\n",
      "{'attention_type', 'dropout'} was not found in config. Values will be initialized to default values.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mphilipp-zettl\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.16.6\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/home/phil/work/mb/easybits/factory/train/wandb/run-20240407_204325-gs0vw9iu\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33musual-plant-7\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ‚≠êÔ∏è View project at \u001b[34m\u001b[4mhttps://wandb.ai/philipp-zettl/dreambooth-lora-sd-xl\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: üöÄ View run at \u001b[34m\u001b[4mhttps://wandb.ai/philipp-zettl/dreambooth-lora-sd-xl/runs/gs0vw9iu\u001b[0m\n",
      "04/07/2024 20:43:26 - INFO - __main__ - ***** Running training *****\n",
      "04/07/2024 20:43:26 - INFO - __main__ -   Num examples = 29\n",
      "04/07/2024 20:43:26 - INFO - __main__ -   Num batches each epoch = 29\n",
      "04/07/2024 20:43:26 - INFO - __main__ -   Num Epochs = 18\n",
      "04/07/2024 20:43:26 - INFO - __main__ -   Instantaneous batch size per device = 1\n",
      "04/07/2024 20:43:26 - INFO - __main__ -   Total train batch size (w. parallel, distributed & accumulation) = 1\n",
      "04/07/2024 20:43:26 - INFO - __main__ -   Gradient Accumulation steps = 1\n",
      "04/07/2024 20:43:26 - INFO - __main__ -   Total optimization steps = 500\n",
      "Steps:  20%|‚ñà‚ñä       | 100/500 [00:32<02:06,  3.15it/s, loss=0.0459, lr=9.06e-5]04/07/2024 20:43:59 - INFO - accelerate.accelerator - Saving current state to ./models/ssd-jon_juarez-full-lora/checkpoint-100\n",
      "Model weights saved in models/ssd-jon_juarez-full-lora/checkpoint-100/pytorch_lora_weights.safetensors\n",
      "04/07/2024 20:43:59 - INFO - accelerate.checkpointing - Optimizer state saved in models/ssd-jon_juarez-full-lora/checkpoint-100/optimizer.bin\n",
      "04/07/2024 20:43:59 - INFO - accelerate.checkpointing - Scheduler state saved in models/ssd-jon_juarez-full-lora/checkpoint-100/scheduler.bin\n",
      "04/07/2024 20:43:59 - INFO - accelerate.checkpointing - Sampler state for dataloader 0 saved in models/ssd-jon_juarez-full-lora/checkpoint-100/sampler.bin\n",
      "04/07/2024 20:43:59 - INFO - accelerate.checkpointing - Gradient scaler state saved in models/ssd-jon_juarez-full-lora/checkpoint-100/scaler.pt\n",
      "04/07/2024 20:43:59 - INFO - accelerate.checkpointing - Random states saved in models/ssd-jon_juarez-full-lora/checkpoint-100/random_states_0.pkl\n",
      "04/07/2024 20:43:59 - INFO - __main__ - Saved state to ./models/ssd-jon_juarez-full-lora/checkpoint-100\n",
      "Steps:  40%|‚ñà‚ñà‚ñà‚ñå     | 200/500 [01:04<01:30,  3.32it/s, loss=0.0429, lr=6.57e-5]04/07/2024 20:44:30 - INFO - accelerate.accelerator - Saving current state to ./models/ssd-jon_juarez-full-lora/checkpoint-200\n",
      "Model weights saved in models/ssd-jon_juarez-full-lora/checkpoint-200/pytorch_lora_weights.safetensors\n",
      "04/07/2024 20:44:31 - INFO - accelerate.checkpointing - Optimizer state saved in models/ssd-jon_juarez-full-lora/checkpoint-200/optimizer.bin\n",
      "04/07/2024 20:44:31 - INFO - accelerate.checkpointing - Scheduler state saved in models/ssd-jon_juarez-full-lora/checkpoint-200/scheduler.bin\n",
      "04/07/2024 20:44:31 - INFO - accelerate.checkpointing - Sampler state for dataloader 0 saved in models/ssd-jon_juarez-full-lora/checkpoint-200/sampler.bin\n",
      "04/07/2024 20:44:31 - INFO - accelerate.checkpointing - Gradient scaler state saved in models/ssd-jon_juarez-full-lora/checkpoint-200/scaler.pt\n",
      "04/07/2024 20:44:31 - INFO - accelerate.checkpointing - Random states saved in models/ssd-jon_juarez-full-lora/checkpoint-200/random_states_0.pkl\n",
      "04/07/2024 20:44:31 - INFO - __main__ - Saved state to ./models/ssd-jon_juarez-full-lora/checkpoint-200\n",
      "Steps:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 300/500 [01:34<00:59,  3.34it/s, loss=0.0031, lr=3.48e-5]04/07/2024 20:45:01 - INFO - accelerate.accelerator - Saving current state to ./models/ssd-jon_juarez-full-lora/checkpoint-300\n",
      "Model weights saved in models/ssd-jon_juarez-full-lora/checkpoint-300/pytorch_lora_weights.safetensors\n",
      "04/07/2024 20:45:01 - INFO - accelerate.checkpointing - Optimizer state saved in models/ssd-jon_juarez-full-lora/checkpoint-300/optimizer.bin\n",
      "04/07/2024 20:45:01 - INFO - accelerate.checkpointing - Scheduler state saved in models/ssd-jon_juarez-full-lora/checkpoint-300/scheduler.bin\n",
      "04/07/2024 20:45:01 - INFO - accelerate.checkpointing - Sampler state for dataloader 0 saved in models/ssd-jon_juarez-full-lora/checkpoint-300/sampler.bin\n",
      "04/07/2024 20:45:01 - INFO - accelerate.checkpointing - Gradient scaler state saved in models/ssd-jon_juarez-full-lora/checkpoint-300/scaler.pt\n",
      "04/07/2024 20:45:01 - INFO - accelerate.checkpointing - Random states saved in models/ssd-jon_juarez-full-lora/checkpoint-300/random_states_0.pkl\n",
      "04/07/2024 20:45:01 - INFO - __main__ - Saved state to ./models/ssd-jon_juarez-full-lora/checkpoint-300\n",
      "Steps:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 400/500 [02:05<00:30,  3.30it/s, loss=0.196, lr=9.73e-6]04/07/2024 20:45:31 - INFO - accelerate.accelerator - Saving current state to ./models/ssd-jon_juarez-full-lora/checkpoint-400\n",
      "Model weights saved in models/ssd-jon_juarez-full-lora/checkpoint-400/pytorch_lora_weights.safetensors\n",
      "04/07/2024 20:45:32 - INFO - accelerate.checkpointing - Optimizer state saved in models/ssd-jon_juarez-full-lora/checkpoint-400/optimizer.bin\n",
      "04/07/2024 20:45:32 - INFO - accelerate.checkpointing - Scheduler state saved in models/ssd-jon_juarez-full-lora/checkpoint-400/scheduler.bin\n",
      "04/07/2024 20:45:32 - INFO - accelerate.checkpointing - Sampler state for dataloader 0 saved in models/ssd-jon_juarez-full-lora/checkpoint-400/sampler.bin\n",
      "04/07/2024 20:45:32 - INFO - accelerate.checkpointing - Gradient scaler state saved in models/ssd-jon_juarez-full-lora/checkpoint-400/scaler.pt\n",
      "04/07/2024 20:45:32 - INFO - accelerate.checkpointing - Random states saved in models/ssd-jon_juarez-full-lora/checkpoint-400/random_states_0.pkl\n",
      "04/07/2024 20:45:32 - INFO - __main__ - Saved state to ./models/ssd-jon_juarez-full-lora/checkpoint-400\n",
      "Steps: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 500/500 [02:35<00:00,  3.30it/s, loss=0.067, lr=9.87e-10]04/07/2024 20:46:02 - INFO - accelerate.accelerator - Saving current state to ./models/ssd-jon_juarez-full-lora/checkpoint-500\n",
      "Model weights saved in models/ssd-jon_juarez-full-lora/checkpoint-500/pytorch_lora_weights.safetensors\n",
      "04/07/2024 20:46:02 - INFO - accelerate.checkpointing - Optimizer state saved in models/ssd-jon_juarez-full-lora/checkpoint-500/optimizer.bin\n",
      "04/07/2024 20:46:02 - INFO - accelerate.checkpointing - Scheduler state saved in models/ssd-jon_juarez-full-lora/checkpoint-500/scheduler.bin\n",
      "04/07/2024 20:46:02 - INFO - accelerate.checkpointing - Sampler state for dataloader 0 saved in models/ssd-jon_juarez-full-lora/checkpoint-500/sampler.bin\n",
      "04/07/2024 20:46:02 - INFO - accelerate.checkpointing - Gradient scaler state saved in models/ssd-jon_juarez-full-lora/checkpoint-500/scaler.pt\n",
      "04/07/2024 20:46:03 - INFO - accelerate.checkpointing - Random states saved in models/ssd-jon_juarez-full-lora/checkpoint-500/random_states_0.pkl\n",
      "04/07/2024 20:46:03 - INFO - __main__ - Saved state to ./models/ssd-jon_juarez-full-lora/checkpoint-500\n",
      "Steps: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 500/500 [02:36<00:00,  3.30it/s, loss=0.378, lr=0]Model weights saved in models/ssd-jon_juarez-full-lora/pytorch_lora_weights.safetensors\n",
      "{'latents_mean', 'latents_std'} was not found in config. Values will be initialized to default values.\n",
      "{'feature_extractor', 'image_encoder', 'add_watermarker'} was not found in config. Values will be initialized to default values.\n",
      "\n",
      "Loading pipeline components...:   0%|                     | 0/7 [00:00<?, ?it/s]\u001b[ALoaded text_encoder as CLIPTextModel from `text_encoder` subfolder of segmind/SSD-1B.\n",
      "{'sigma_min', 'sigma_max', 'rescale_betas_zero_snr', 'timestep_type'} was not found in config. Values will be initialized to default values.\n",
      "Loaded scheduler as EulerDiscreteScheduler from `scheduler` subfolder of segmind/SSD-1B.\n",
      "Loaded tokenizer as CLIPTokenizer from `tokenizer` subfolder of segmind/SSD-1B.\n",
      "\n",
      "Loading pipeline components...:  43%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå       | 3/7 [00:00<00:00, 29.91it/s]\u001b[ALoaded tokenizer_2 as CLIPTokenizer from `tokenizer_2` subfolder of segmind/SSD-1B.\n",
      "{'attention_type', 'dropout'} was not found in config. Values will be initialized to default values.\n",
      "Loaded unet as UNet2DConditionModel from `unet` subfolder of segmind/SSD-1B.\n",
      "Loaded text_encoder_2 as CLIPTextModelWithProjection from `text_encoder_2` subfolder of segmind/SSD-1B.\n",
      "\n",
      "Loading pipeline components...: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 7/7 [00:00<00:00, 23.87it/s]\u001b[A\n",
      "Loading unet.\n",
      "\n",
      "optimizer.bin:   0%|                                | 0.00/22.1M [00:00<?, ?B/s]\u001b[A\n",
      "\n",
      "random_states_0.pkl:   0%|                          | 0.00/14.3k [00:00<?, ?B/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "pytorch_lora_weights.safetensors:   0%|             | 0.00/10.9M [00:00<?, ?B/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "scaler.pt:   0%|                                      | 0.00/988 [00:00<?, ?B/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "scheduler.bin:   0%|                                | 0.00/1.00k [00:00<?, ?B/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Upload 26 LFS files:   0%|                               | 0/26 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "scaler.pt: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 988/988 [00:00<00:00, 8.05kB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "optimizer.bin:   0%|                        | 16.4k/22.1M [00:00<03:07, 118kB/s]\u001b[A\n",
      "\n",
      "\n",
      "pytorch_lora_weights.safetensors:   0%|     | 16.4k/10.9M [00:00<01:28, 123kB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "scheduler.bin: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1.00k/1.00k [00:00<00:00, 6.00kB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "random_states_0.pkl: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 14.3k/14.3k [00:00<00:00, 73.2kB/s]\u001b[A\u001b[A\n",
      "optimizer.bin:   1%|‚ñè                        | 131k/22.1M [00:00<00:39, 551kB/s]\u001b[A\n",
      "\n",
      "\n",
      "pytorch_lora_weights.safetensors:   1%|      | 131k/10.9M [00:00<00:21, 503kB/s]\u001b[A\u001b[A\u001b[A\n",
      "scaler.pt: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 988/988 [00:00<00:00, 2.59kB/s]\u001b[A\n",
      "scheduler.bin: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1.00k/1.00k [00:00<00:00, 2.32kB/s]\n",
      "\n",
      "\n",
      "\n",
      "pytorch_lora_weights.safetensors:   8%|‚ñç    | 836k/10.9M [00:00<00:05, 1.78MB/s]\u001b[A\u001b[A\u001b[A\n",
      "random_states_0.pkl: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 14.3k/14.3k [00:00<00:00, 19.9kB/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "pytorch_lora_weights.safetensors:  11%|‚ñç   | 1.16M/10.9M [00:00<00:05, 1.80MB/s]\u001b[A\u001b[A\u001b[A\n",
      "optimizer.bin:   7%|‚ñà‚ñå                     | 1.54M/22.1M [00:00<00:08, 2.38MB/s]\u001b[A\n",
      "\n",
      "optimizer.bin:   0%|                                | 0.00/22.1M [00:00<?, ?B/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "pytorch_lora_weights.safetensors:   0%|             | 0.00/10.9M [00:00<?, ?B/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "pytorch_lora_weights.safetensors:   1%|     | 98.3k/10.9M [00:00<00:13, 774kB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "optimizer.bin:   1%|‚ñè                        | 115k/22.1M [00:00<00:31, 705kB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "pytorch_lora_weights.safetensors:   2%|      | 180k/10.9M [00:00<00:13, 785kB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "optimizer.bin:   1%|‚ñè                        | 197k/22.1M [00:00<00:29, 741kB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "pytorch_lora_weights.safetensors:   4%|‚ñè    | 475k/10.9M [00:00<00:06, 1.57MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "optimizer.bin:   2%|‚ñå                       | 508k/22.1M [00:00<00:13, 1.56MB/s]\u001b[A\u001b[A\n",
      "optimizer.bin:   9%|‚ñà‚ñà                     | 1.93M/22.1M [00:01<00:16, 1.25MB/s]\u001b[A\n",
      "\n",
      "\n",
      "pytorch_lora_weights.safetensors:  14%|‚ñã    | 1.49M/10.9M [00:01<00:09, 953kB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "optimizer.bin:   3%|‚ñä                       | 770k/22.1M [00:00<00:11, 1.83MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "random_states_0.pkl:   0%|                          | 0.00/14.3k [00:00<?, ?B/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "pytorch_lora_weights.safetensors:   6%|‚ñé    | 672k/10.9M [00:00<00:07, 1.35MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "pytorch_lora_weights.safetensors:   9%|‚ñç    | 999k/10.9M [00:00<00:06, 1.65MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "optimizer.bin:   6%|‚ñà‚ñç                     | 1.41M/22.1M [00:00<00:09, 2.22MB/s]\u001b[A\u001b[A\n",
      "random_states_0.pkl: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 14.3k/14.3k [00:00<00:00, 31.1kB/s]\u001b[A\n",
      "\n",
      "\n",
      "optimizer.bin:   8%|‚ñà‚ñä                     | 1.80M/22.1M [00:00<00:10, 1.96MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "pytorch_lora_weights.safetensors:  11%|‚ñç   | 1.25M/10.9M [00:00<00:07, 1.25MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "pytorch_lora_weights.safetensors:  17%|‚ñä    | 1.80M/10.9M [00:01<00:11, 816kB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "scaler.pt:   0%|                                      | 0.00/988 [00:00<?, ?B/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "pytorch_lora_weights.safetensors:  14%|‚ñã    | 1.51M/10.9M [00:01<00:14, 655kB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "scaler.pt: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 988/988 [00:00<00:00, 1.63kB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "pytorch_lora_weights.safetensors:  16%|‚ñä    | 1.77M/10.9M [00:01<00:11, 764kB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "optimizer.bin:  12%|‚ñà‚ñà‚ñâ                     | 2.70M/22.1M [00:02<00:29, 647kB/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "scheduler.bin:   0%|                                | 0.00/1.00k [00:00<?, ?B/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "scheduler.bin: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1.00k/1.00k [00:00<00:00, 3.40kB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "optimizer.bin:  11%|‚ñà‚ñà‚ñã                     | 2.44M/22.1M [00:02<00:30, 647kB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "optimizer.bin:   0%|                                | 0.00/22.1M [00:00<?, ?B/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "pytorch_lora_weights.safetensors:  21%|‚ñà    | 2.29M/10.9M [00:02<00:14, 599kB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "pytorch_lora_weights.safetensors:  20%|‚ñâ    | 2.13M/10.9M [00:03<00:24, 359kB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "optimizer.bin:   1%|‚ñè                        | 164k/22.1M [00:00<00:40, 538kB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "optimizer.bin:   1%|‚ñé                        | 246k/22.1M [00:00<00:40, 546kB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "optimizer.bin:  13%|‚ñà‚ñà‚ñà                     | 2.77M/22.1M [00:03<00:32, 591kB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "pytorch_lora_weights.safetensors:  24%|‚ñà‚ñè   | 2.61M/10.9M [00:03<00:12, 644kB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "optimizer.bin:   1%|‚ñé                        | 328k/22.1M [00:00<00:48, 446kB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "optimizer.bin:  14%|‚ñà‚ñà‚ñà‚ñé                    | 3.10M/22.1M [00:04<00:44, 423kB/s]\u001b[A\n",
      "\n",
      "\n",
      "pytorch_lora_weights.safetensors:  23%|‚ñà‚ñè   | 2.46M/10.9M [00:04<00:21, 396kB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "optimizer.bin:   2%|‚ñç                        | 393k/22.1M [00:00<01:02, 348kB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "optimizer.bin:  14%|‚ñà‚ñà‚ñà‚ñé                    | 3.10M/22.1M [00:03<00:32, 589kB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "optimizer.bin:   2%|‚ñå                        | 459k/22.1M [00:01<01:18, 276kB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "pytorch_lora_weights.safetensors:  26%|‚ñà‚ñé   | 2.87M/10.9M [00:04<00:16, 482kB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "optimizer.bin:   2%|‚ñå                        | 541k/22.1M [00:01<01:50, 195kB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "pytorch_lora_weights.safetensors:  26%|‚ñà‚ñé   | 2.79M/10.9M [00:05<00:23, 348kB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "optimizer.bin:  15%|‚ñà‚ñà‚ñà‚ñã                    | 3.41M/22.1M [00:04<00:39, 469kB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "optimizer.bin:   3%|‚ñã                        | 623k/22.1M [00:02<01:55, 187kB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "optimizer.bin:  16%|‚ñà‚ñà‚ñà‚ñä                    | 3.47M/22.1M [00:06<00:58, 321kB/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "optimizer.bin:   3%|‚ñä                        | 688k/22.1M [00:02<01:37, 221kB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "optimizer.bin:   3%|‚ñä                        | 754k/22.1M [00:02<01:35, 223kB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "pytorch_lora_weights.safetensors:  29%|‚ñà‚ñç   | 3.13M/10.9M [00:05<00:24, 317kB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "optimizer.bin:  17%|‚ñà‚ñà‚ñà‚ñà                    | 3.74M/22.1M [00:05<00:44, 414kB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "optimizer.bin:   4%|‚ñâ                        | 819k/22.1M [00:03<01:26, 247kB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "pytorch_lora_weights.safetensors:  28%|‚ñà‚ñç   | 3.10M/10.9M [00:06<00:25, 312kB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "optimizer.bin:   4%|‚ñà                        | 901k/22.1M [00:03<01:21, 262kB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "optimizer.bin:   4%|‚ñà                        | 983k/22.1M [00:03<01:31, 231kB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "pytorch_lora_weights.safetensors:  31%|‚ñà‚ñå   | 3.39M/10.9M [00:06<00:26, 280kB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "optimizer.bin:   5%|‚ñà‚ñè                      | 1.05M/22.1M [00:04<02:31, 139kB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "optimizer.bin:  17%|‚ñà‚ñà‚ñà‚ñà‚ñè                   | 3.87M/22.1M [00:08<01:17, 236kB/s]\u001b[A\n",
      "\n",
      "\n",
      "pytorch_lora_weights.safetensors:  31%|‚ñà‚ñå   | 3.36M/10.9M [00:09<00:33, 225kB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "optimizer.bin:  18%|‚ñà‚ñà‚ñà‚ñà‚ñç                   | 4.06M/22.1M [00:08<01:08, 263kB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "optimizer.bin:   5%|‚ñà‚ñè                      | 1.13M/22.1M [00:05<02:33, 137kB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "optimizer.bin:   5%|‚ñà‚ñé                      | 1.20M/22.1M [00:05<02:30, 139kB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "optimizer.bin:   6%|‚ñà‚ñé                      | 1.26M/22.1M [00:06<02:16, 153kB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "pytorch_lora_weights.safetensors:  33%|‚ñà‚ñã   | 3.64M/10.9M [00:08<00:35, 203kB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "optimizer.bin:   6%|‚ñà‚ñç                      | 1.33M/22.1M [00:06<02:21, 147kB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "optimizer.bin:   6%|‚ñà‚ñå                      | 1.41M/22.1M [00:06<02:00, 172kB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "pytorch_lora_weights.safetensors:  34%|‚ñà‚ñã   | 3.69M/10.9M [00:10<00:34, 209kB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "optimizer.bin:  20%|‚ñà‚ñà‚ñà‚ñà‚ñä                   | 4.39M/22.1M [00:10<01:17, 229kB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "optimizer.bin:   7%|‚ñà‚ñã                      | 1.54M/22.1M [00:07<01:35, 216kB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "optimizer.bin:   7%|‚ñà‚ñã                      | 1.61M/22.1M [00:07<01:22, 247kB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "pytorch_lora_weights.safetensors:  36%|‚ñà‚ñä   | 3.90M/10.9M [00:10<00:35, 198kB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "optimizer.bin:   8%|‚ñà‚ñä                      | 1.67M/22.1M [00:08<01:45, 193kB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "optimizer.bin:  19%|‚ñà‚ñà‚ñà‚ñà‚ñå                   | 4.26M/22.1M [00:11<01:34, 190kB/s]\u001b[A\n",
      "\n",
      "\n",
      "pytorch_lora_weights.safetensors:  37%|‚ñà‚ñä   | 4.01M/10.9M [00:11<00:30, 228kB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "optimizer.bin:  21%|‚ñà‚ñà‚ñà‚ñà‚ñà                   | 4.70M/22.1M [00:11<01:11, 243kB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "optimizer.bin:   8%|‚ñà‚ñâ                      | 1.74M/22.1M [00:08<01:47, 190kB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "optimizer.bin:   8%|‚ñà‚ñâ                      | 1.80M/22.1M [00:08<01:53, 178kB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "pytorch_lora_weights.safetensors:  38%|‚ñà‚ñâ   | 4.16M/10.9M [00:11<00:35, 192kB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "optimizer.bin:   8%|‚ñà‚ñà                      | 1.85M/22.1M [00:09<01:52, 180kB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "optimizer.bin:  23%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                  | 5.03M/22.1M [00:12<01:05, 263kB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "optimizer.bin:   9%|‚ñà‚ñà                      | 1.92M/22.1M [00:09<01:39, 203kB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "pytorch_lora_weights.safetensors:  39%|‚ñà‚ñâ   | 4.26M/10.9M [00:13<00:29, 224kB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "optimizer.bin:   9%|‚ñà‚ñà‚ñè                     | 1.98M/22.1M [00:09<01:52, 179kB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "optimizer.bin:   9%|‚ñà‚ñà‚ñè                     | 2.05M/22.1M [00:09<01:28, 228kB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "pytorch_lora_weights.safetensors:  41%|‚ñà‚ñà   | 4.42M/10.9M [00:12<00:31, 207kB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "optimizer.bin:  10%|‚ñà‚ñà‚ñé                     | 2.11M/22.1M [00:10<01:39, 202kB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "optimizer.bin:  21%|‚ñà‚ñà‚ñà‚ñà‚ñà                   | 4.65M/22.1M [00:14<01:32, 189kB/s]\u001b[A\n",
      "\n",
      "optimizer.bin:  24%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                  | 5.36M/22.1M [00:13<01:03, 265kB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "optimizer.bin:  10%|‚ñà‚ñà‚ñé                     | 2.18M/22.1M [00:10<01:26, 231kB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "pytorch_lora_weights.safetensors:  42%|‚ñà‚ñà   | 4.59M/10.9M [00:14<00:26, 238kB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "optimizer.bin:  10%|‚ñà‚ñà‚ñç                     | 2.24M/22.1M [00:10<01:13, 271kB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "pytorch_lora_weights.safetensors:  43%|‚ñà‚ñà‚ñè  | 4.69M/10.9M [00:13<00:25, 240kB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "optimizer.bin:  10%|‚ñà‚ñà‚ñå                     | 2.31M/22.1M [00:10<01:07, 295kB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "optimizer.bin:  11%|‚ñà‚ñà‚ñå                     | 2.38M/22.1M [00:11<01:42, 192kB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "pytorch_lora_weights.safetensors:  44%|‚ñà‚ñà‚ñè  | 4.85M/10.9M [00:15<00:25, 241kB/s]\u001b[A\u001b[A\u001b[A\n",
      "optimizer.bin:  23%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                  | 5.03M/22.1M [00:15<01:21, 210kB/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "optimizer.bin:  11%|‚ñà‚ñà‚ñã                     | 2.44M/22.1M [00:11<01:30, 218kB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "optimizer.bin:  26%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                 | 5.69M/22.1M [00:14<01:05, 253kB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "optimizer.bin:  11%|‚ñà‚ñà‚ñã                     | 2.51M/22.1M [00:11<01:27, 225kB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "optimizer.bin:  12%|‚ñà‚ñà‚ñä                     | 2.57M/22.1M [00:12<01:14, 262kB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "pytorch_lora_weights.safetensors:  45%|‚ñà‚ñà‚ñé  | 4.93M/10.9M [00:14<00:27, 220kB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "pytorch_lora_weights.safetensors:  47%|‚ñà‚ñà‚ñé  | 5.11M/10.9M [00:15<00:20, 288kB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "optimizer.bin:  12%|‚ñà‚ñà‚ñä                     | 2.64M/22.1M [00:12<01:49, 178kB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "optimizer.bin:  12%|‚ñà‚ñà‚ñâ                     | 2.70M/22.1M [00:12<01:25, 227kB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "optimizer.bin:  13%|‚ñà‚ñà‚ñà                     | 2.77M/22.1M [00:13<01:13, 262kB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "optimizer.bin:  13%|‚ñà‚ñà‚ñà                     | 2.83M/22.1M [00:13<01:22, 235kB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "pytorch_lora_weights.safetensors:  50%|‚ñà‚ñà‚ñç  | 5.42M/10.9M [00:17<00:20, 273kB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "optimizer.bin:  27%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                 | 6.00M/22.1M [00:16<01:10, 228kB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "optimizer.bin:  13%|‚ñà‚ñà‚ñà‚ñè                    | 2.90M/22.1M [00:13<01:35, 202kB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "pytorch_lora_weights.safetensors:  48%|‚ñà‚ñà‚ñç  | 5.19M/10.9M [00:16<00:29, 193kB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "optimizer.bin:  13%|‚ñà‚ñà‚ñà‚ñè                    | 2.97M/22.1M [00:13<01:22, 233kB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "optimizer.bin:  14%|‚ñà‚ñà‚ñà‚ñé                    | 3.03M/22.1M [00:14<01:07, 282kB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "optimizer.bin:  25%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                  | 5.42M/22.1M [00:17<01:26, 192kB/s]\u001b[A\n",
      "\n",
      "\n",
      "pytorch_lora_weights.safetensors:  53%|‚ñà‚ñà‚ñã  | 5.75M/10.9M [00:17<00:17, 301kB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "optimizer.bin:  14%|‚ñà‚ñà‚ñà‚ñé                    | 3.10M/22.1M [00:14<01:03, 298kB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "optimizer.bin:  14%|‚ñà‚ñà‚ñà‚ñç                    | 3.16M/22.1M [00:14<01:28, 214kB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "optimizer.bin:  15%|‚ñà‚ñà‚ñà‚ñå                    | 3.23M/22.1M [00:14<01:12, 259kB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "optimizer.bin:  15%|‚ñà‚ñà‚ñà‚ñã                    | 3.34M/22.1M [00:15<00:55, 341kB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "optimizer.bin:  29%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                 | 6.32M/22.1M [00:18<01:12, 219kB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "optimizer.bin:  15%|‚ñà‚ñà‚ñà‚ñã                    | 3.41M/22.1M [00:15<00:53, 349kB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "pytorch_lora_weights.safetensors:  50%|‚ñà‚ñà‚ñå  | 5.46M/10.9M [00:18<00:28, 189kB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "pytorch_lora_weights.safetensors:  55%|‚ñà‚ñà‚ñä  | 6.01M/10.9M [00:19<00:19, 250kB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "optimizer.bin:  16%|‚ñà‚ñà‚ñà‚ñä                    | 3.47M/22.1M [00:15<01:19, 235kB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "optimizer.bin:  26%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                 | 5.82M/22.1M [00:19<01:21, 201kB/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "optimizer.bin:  16%|‚ñà‚ñà‚ñà‚ñä                    | 3.54M/22.1M [00:15<01:08, 270kB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "optimizer.bin:  16%|‚ñà‚ñà‚ñà‚ñâ                    | 3.60M/22.1M [00:16<01:06, 280kB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "optimizer.bin:  17%|‚ñà‚ñà‚ñà‚ñâ                    | 3.67M/22.1M [00:16<01:02, 293kB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "optimizer.bin:  17%|‚ñà‚ñà‚ñà‚ñà                    | 3.74M/22.1M [00:16<00:56, 328kB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "pytorch_lora_weights.safetensors:  52%|‚ñà‚ñà‚ñå  | 5.72M/10.9M [00:19<00:28, 184kB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "optimizer.bin:  28%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                 | 6.19M/22.1M [00:20<01:07, 235kB/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "optimizer.bin:  17%|‚ñà‚ñà‚ñà‚ñà‚ñè                   | 3.87M/22.1M [00:16<00:50, 358kB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "optimizer.bin:  30%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                | 6.65M/22.1M [00:19<01:14, 207kB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "optimizer.bin:  18%|‚ñà‚ñà‚ñà‚ñà‚ñé                   | 4.00M/22.1M [00:17<00:42, 426kB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "pytorch_lora_weights.safetensors:  58%|‚ñà‚ñà‚ñâ  | 6.34M/10.9M [00:20<00:18, 246kB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "pytorch_lora_weights.safetensors:  55%|‚ñà‚ñà‚ñã  | 5.98M/10.9M [00:20<00:21, 231kB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "optimizer.bin:  19%|‚ñà‚ñà‚ñà‚ñà‚ñå                   | 4.19M/22.1M [00:17<00:31, 565kB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "optimizer.bin:  19%|‚ñà‚ñà‚ñà‚ñà‚ñå                   | 4.26M/22.1M [00:17<00:31, 567kB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "optimizer.bin:  30%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                | 6.59M/22.1M [00:21<00:56, 276kB/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "pytorch_lora_weights.safetensors:  57%|‚ñà‚ñà‚ñä  | 6.23M/10.9M [00:20<00:17, 270kB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "optimizer.bin:  20%|‚ñà‚ñà‚ñà‚ñà‚ñä                   | 4.39M/22.1M [00:17<00:39, 447kB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "optimizer.bin:  20%|‚ñà‚ñà‚ñà‚ñà‚ñä                   | 4.46M/22.1M [00:17<00:39, 446kB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "optimizer.bin:  32%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                | 6.98M/22.1M [00:20<01:06, 228kB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "optimizer.bin:  21%|‚ñà‚ñà‚ñà‚ñà‚ñâ                   | 4.57M/22.1M [00:18<00:57, 304kB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "pytorch_lora_weights.safetensors:  59%|‚ñà‚ñà‚ñâ  | 6.49M/10.9M [00:21<00:15, 283kB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "pytorch_lora_weights.safetensors:  61%|‚ñà‚ñà‚ñà  | 6.65M/10.9M [00:22<00:18, 225kB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "optimizer.bin:  21%|‚ñà‚ñà‚ñà‚ñà‚ñà                   | 4.64M/22.1M [00:18<00:59, 294kB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "optimizer.bin:  32%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                | 6.98M/22.1M [00:22<00:54, 278kB/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "optimizer.bin:  22%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                  | 4.77M/22.1M [00:19<00:52, 329kB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "pytorch_lora_weights.safetensors:  62%|‚ñà‚ñà‚ñà  | 6.75M/10.9M [00:21<00:13, 319kB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "optimizer.bin:  33%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                | 7.29M/22.1M [00:22<01:02, 238kB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "optimizer.bin:  22%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                  | 4.90M/22.1M [00:19<00:48, 354kB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "optimizer.bin:  23%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                  | 5.03M/22.1M [00:19<00:40, 424kB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "pytorch_lora_weights.safetensors:  64%|‚ñà‚ñà‚ñà‚ñè | 7.01M/10.9M [00:22<00:10, 370kB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "optimizer.bin:  23%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                  | 5.16M/22.1M [00:20<00:52, 325kB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "pytorch_lora_weights.safetensors:  64%|‚ñà‚ñà‚ñà‚ñè | 6.98M/10.9M [00:24<00:18, 214kB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "optimizer.bin:  24%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                  | 5.29M/22.1M [00:20<00:46, 360kB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "optimizer.bin:  25%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                  | 5.42M/22.1M [00:20<00:46, 363kB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "pytorch_lora_weights.safetensors:  67%|‚ñà‚ñà‚ñà‚ñé | 7.27M/10.9M [00:23<00:12, 300kB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "optimizer.bin:  33%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                | 7.36M/22.1M [00:24<00:58, 251kB/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "optimizer.bin:  26%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                 | 5.69M/22.1M [00:21<00:30, 546kB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "optimizer.bin:  27%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                 | 5.87M/22.1M [00:21<00:26, 622kB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "pytorch_lora_weights.safetensors:  67%|‚ñà‚ñà‚ñà‚ñé | 7.31M/10.9M [00:25<00:14, 247kB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "pytorch_lora_weights.safetensors:  69%|‚ñà‚ñà‚ñà‚ñç | 7.52M/10.9M [00:24<00:10, 338kB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "optimizer.bin:  27%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                 | 6.00M/22.1M [00:21<00:23, 699kB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "optimizer.bin:  34%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé               | 7.62M/22.1M [00:24<01:12, 199kB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "optimizer.bin:  28%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                 | 6.13M/22.1M [00:22<00:47, 336kB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "pytorch_lora_weights.safetensors:  71%|‚ñà‚ñà‚ñà‚ñå | 7.78M/10.9M [00:25<00:11, 271kB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "optimizer.bin:  35%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç               | 7.75M/22.1M [00:26<01:01, 235kB/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "optimizer.bin:  28%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                 | 6.26M/22.1M [00:22<00:50, 316kB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "pytorch_lora_weights.safetensors:  70%|‚ñà‚ñà‚ñà‚ñç | 7.63M/10.9M [00:26<00:15, 217kB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "optimizer.bin:  29%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                 | 6.46M/22.1M [00:23<00:43, 356kB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "optimizer.bin:  36%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå               | 7.95M/22.1M [00:26<01:13, 192kB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "pytorch_lora_weights.safetensors:  74%|‚ñà‚ñà‚ñà‚ñã | 8.04M/10.9M [00:26<00:09, 307kB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "optimizer.bin:  37%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä               | 8.14M/22.1M [00:27<00:48, 290kB/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "optimizer.bin:  30%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                | 6.59M/22.1M [00:23<00:45, 342kB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "pytorch_lora_weights.safetensors:  73%|‚ñà‚ñà‚ñà‚ñã | 7.95M/10.9M [00:27<00:11, 258kB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "optimizer.bin:  37%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ               | 8.27M/22.1M [00:26<00:57, 239kB/s]\u001b[A\u001b[A\n",
      "optimizer.bin:  39%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè              | 8.52M/22.1M [00:27<00:39, 348kB/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "optimizer.bin:  30%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                | 6.72M/22.1M [00:24<00:45, 340kB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "pytorch_lora_weights.safetensors:  76%|‚ñà‚ñà‚ñà‚ñä | 8.31M/10.9M [00:26<00:08, 325kB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "optimizer.bin:  31%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                | 6.85M/22.1M [00:25<01:17, 198kB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "pytorch_lora_weights.safetensors:  76%|‚ñà‚ñà‚ñà‚ñä | 8.27M/10.9M [00:29<00:11, 239kB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "optimizer.bin:  39%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé              | 8.59M/22.1M [00:28<01:00, 222kB/s]\u001b[A\u001b[A\n",
      "optimizer.bin:  40%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã              | 8.91M/22.1M [00:29<00:43, 306kB/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "pytorch_lora_weights.safetensors:  79%|‚ñà‚ñà‚ñà‚ñâ | 8.57M/10.9M [00:28<00:10, 234kB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "optimizer.bin:  32%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                | 6.98M/22.1M [00:26<01:12, 209kB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "pytorch_lora_weights.safetensors:  79%|‚ñà‚ñà‚ñà‚ñâ | 8.60M/10.9M [00:29<00:07, 294kB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "optimizer.bin:  40%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã              | 8.91M/22.1M [00:29<00:52, 250kB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "optimizer.bin:  32%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                | 7.09M/22.1M [00:26<01:17, 194kB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "optimizer.bin:  42%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà              | 9.31M/22.1M [00:30<00:40, 318kB/s]\u001b[A\n",
      "\n",
      "\n",
      "pytorch_lora_weights.safetensors:  82%|‚ñà‚ñà‚ñà‚ñà | 8.93M/10.9M [00:30<00:06, 329kB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "pytorch_lora_weights.safetensors:  81%|‚ñà‚ñà‚ñà‚ñà | 8.81M/10.9M [00:29<00:08, 242kB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "optimizer.bin:  33%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                | 7.23M/22.1M [00:27<01:06, 224kB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "pytorch_lora_weights.safetensors:  85%|‚ñà‚ñà‚ñà‚ñà‚ñè| 9.24M/10.9M [00:30<00:04, 382kB/s]\u001b[A\u001b[A\u001b[A\n",
      "optimizer.bin:  44%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå             | 9.68M/22.1M [00:31<00:32, 381kB/s]\u001b[A\n",
      "\n",
      "optimizer.bin:  42%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà              | 9.24M/22.1M [00:30<00:44, 288kB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "optimizer.bin:  33%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                | 7.36M/22.1M [00:27<00:53, 275kB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "pytorch_lora_weights.safetensors:  83%|‚ñà‚ñà‚ñà‚ñà‚ñè| 9.08M/10.9M [00:30<00:06, 289kB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "optimizer.bin:  34%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                | 7.49M/22.1M [00:27<00:40, 358kB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "optimizer.bin:  46%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ             | 10.1M/22.1M [00:31<00:23, 508kB/s]\u001b[A\n",
      "\n",
      "\n",
      "pytorch_lora_weights.safetensors:  88%|‚ñà‚ñà‚ñà‚ñà‚ñç| 9.57M/10.9M [00:31<00:02, 486kB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "optimizer.bin:  34%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé               | 7.62M/22.1M [00:27<00:32, 442kB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "optimizer.bin:  47%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé            | 10.5M/22.1M [00:31<00:18, 624kB/s]\u001b[A\n",
      "\n",
      "optimizer.bin:  43%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé             | 9.55M/22.1M [00:30<00:36, 343kB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "optimizer.bin:  35%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç               | 7.75M/22.1M [00:27<00:31, 457kB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "pytorch_lora_weights.safetensors:  91%|‚ñà‚ñà‚ñà‚ñà‚ñå| 9.90M/10.9M [00:32<00:02, 422kB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "pytorch_lora_weights.safetensors:  86%|‚ñà‚ñà‚ñà‚ñà‚ñé| 9.34M/10.9M [00:31<00:05, 267kB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "optimizer.bin:  36%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå               | 7.95M/22.1M [00:28<00:49, 284kB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "optimizer.bin:  45%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã             | 9.88M/22.1M [00:31<00:39, 307kB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "optimizer.bin:  37%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä               | 8.08M/22.1M [00:29<00:42, 330kB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "pytorch_lora_weights.safetensors:  94%|‚ñà‚ñà‚ñà‚ñà‚ñã| 10.2M/10.9M [00:32<00:01, 440kB/s]\u001b[A\u001b[A\u001b[A\n",
      "optimizer.bin:  49%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä            | 10.9M/22.1M [00:33<00:25, 439kB/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "optimizer.bin:  37%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ               | 8.21M/22.1M [00:29<00:34, 401kB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "optimizer.bin:  38%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà               | 8.32M/22.1M [00:29<00:29, 473kB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "optimizer.bin:  38%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè              | 8.45M/22.1M [00:29<00:24, 555kB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "pytorch_lora_weights.safetensors:  88%|‚ñà‚ñà‚ñà‚ñà‚ñç| 9.60M/10.9M [00:32<00:05, 261kB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "optimizer.bin:  46%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà             | 10.2M/22.1M [00:32<00:32, 367kB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "optimizer.bin:  40%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå              | 8.78M/22.1M [00:29<00:15, 839kB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "pytorch_lora_weights.safetensors:  97%|‚ñà‚ñà‚ñà‚ñà‚ñä| 10.5M/10.9M [00:33<00:00, 461kB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "optimizer.bin:  41%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä              | 9.04M/22.1M [00:30<00:16, 784kB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "optimizer.bin:  51%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè           | 11.2M/22.1M [00:34<00:28, 382kB/s]\u001b[A\n",
      "\n",
      "optimizer.bin:  48%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç            | 10.5M/22.1M [00:33<00:33, 344kB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "optimizer.bin:  42%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà              | 9.24M/22.1M [00:30<00:24, 527kB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "pytorch_lora_weights.safetensors:  90%|‚ñà‚ñà‚ñà‚ñà‚ñå| 9.85M/10.9M [00:33<00:04, 231kB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "optimizer.bin:  43%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè             | 9.44M/22.1M [00:31<00:24, 518kB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "pytorch_lora_weights.safetensors: 100%|‚ñà‚ñà‚ñà‚ñà‚ñâ| 10.9M/10.9M [00:34<00:00, 349kB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "optimizer.bin:  49%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä            | 10.8M/22.1M [00:34<00:31, 362kB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "optimizer.bin:  43%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç             | 9.62M/22.1M [00:31<00:27, 452kB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "pytorch_lora_weights.safetensors:  93%|‚ñà‚ñà‚ñà‚ñà‚ñã| 10.1M/10.9M [00:34<00:03, 253kB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "optimizer.bin:  51%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè           | 11.2M/22.1M [00:34<00:26, 410kB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "optimizer.bin:  44%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã             | 9.81M/22.1M [00:32<00:25, 484kB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "optimizer.bin:  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå           | 11.6M/22.1M [00:35<00:32, 326kB/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "pytorch_lora_weights.safetensors:  95%|‚ñà‚ñà‚ñà‚ñà‚ñä| 10.4M/10.9M [00:35<00:02, 254kB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "optimizer.bin:  45%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä             | 10.0M/22.1M [00:32<00:32, 377kB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch_lora_weights.safetensors: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà| 10.9M/10.9M [00:36<00:00, 295kB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "optimizer.bin:  46%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà             | 10.2M/22.1M [00:33<00:29, 410kB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "pytorch_lora_weights.safetensors:  97%|‚ñà‚ñà‚ñà‚ñà‚ñä| 10.6M/10.9M [00:36<00:00, 297kB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "optimizer.bin:  54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà           | 12.0M/22.1M [00:37<00:30, 331kB/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "optimizer.bin:  47%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé            | 10.4M/22.1M [00:33<00:26, 449kB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "pytorch_lora_weights.safetensors: 100%|‚ñà‚ñà‚ñà‚ñà‚ñâ| 10.9M/10.9M [00:36<00:00, 364kB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "optimizer.bin:  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä           | 11.8M/22.1M [00:36<00:25, 407kB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "optimizer.bin:  48%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå            | 10.6M/22.1M [00:33<00:20, 574kB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "pytorch_lora_weights.safetensors:   0%|             | 0.00/10.9M [00:00<?, ?B/s]\u001b[A\u001b[A\u001b[A\n",
      "optimizer.bin:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç          | 12.4M/22.1M [00:37<00:25, 386kB/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "optimizer.bin:  49%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã            | 10.8M/22.1M [00:34<00:21, 522kB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "optimizer.bin:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ            | 11.0M/22.1M [00:34<00:19, 563kB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch_lora_weights.safetensors: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà| 10.9M/10.9M [00:37<00:00, 291kB/s]\u001b[A\u001b[A\n",
      "\n",
      "optimizer.bin:  58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ          | 12.8M/22.1M [00:38<00:23, 392kB/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "optimizer.bin:  51%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè           | 11.2M/22.1M [00:35<00:24, 447kB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "pytorch_lora_weights.safetensors:   8%|‚ñç     | 819k/10.9M [00:01<00:15, 651kB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "optimizer.bin:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå          | 12.5M/22.1M [00:38<00:23, 407kB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "optimizer.bin:  51%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé           | 11.4M/22.1M [00:35<00:19, 540kB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "optimizer.bin:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé         | 13.2M/22.1M [00:39<00:19, 447kB/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "optimizer.bin:  52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå           | 11.6M/22.1M [00:35<00:17, 606kB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "pytorch_lora_weights.safetensors:  10%|‚ñå    | 1.13M/10.9M [00:01<00:15, 640kB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "random_states_0.pkl:   0%|                          | 0.00/14.3k [00:00<?, ?B/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "optimizer.bin:  54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ           | 12.0M/22.1M [00:35<00:13, 753kB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "optimizer.bin:  58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ          | 12.8M/22.1M [00:38<00:21, 433kB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "optimizer.bin:  55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè          | 12.2M/22.1M [00:36<00:11, 869kB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "random_states_0.pkl: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 14.3k/14.3k [00:00<00:00, 24.3kB/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "optimizer.bin:  58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè         | 12.7M/22.1M [00:36<00:06, 1.34MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "pytorch_lora_weights.safetensors:  13%|‚ñã    | 1.46M/10.9M [00:02<00:16, 565kB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "optimizer.bin:  59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè         | 13.1M/22.1M [00:39<00:18, 481kB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "optimizer.bin:  59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå         | 13.0M/22.1M [00:36<00:06, 1.31MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "scaler.pt:   0%|                                      | 0.00/988 [00:00<?, ?B/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "optimizer.bin:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä         | 13.2M/22.1M [00:36<00:06, 1.34MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "pytorch_lora_weights.safetensors:  16%|‚ñä    | 1.79M/10.9M [00:02<00:14, 631kB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "scaler.pt: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 988/988 [00:00<00:00, 5.18kB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "optimizer.bin:  63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè        | 14.0M/22.1M [00:40<00:15, 512kB/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "optimizer.bin:  63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç        | 13.9M/22.1M [00:36<00:04, 1.67MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "optimizer.bin:  61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå         | 13.4M/22.1M [00:39<00:16, 513kB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "scheduler.bin:   0%|                                | 0.00/1.00k [00:00<?, ?B/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "optimizer.bin:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä        | 14.3M/22.1M [00:37<00:04, 1.90MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "pytorch_lora_weights.safetensors:  19%|‚ñâ    | 2.11M/10.9M [00:03<00:12, 684kB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "optimizer.bin:  66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè       | 14.7M/22.1M [00:37<00:03, 2.13MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "optimizer.bin:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå        | 14.4M/22.1M [00:40<00:13, 580kB/s]\u001b[A\n",
      "\n",
      "optimizer.bin:  62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ         | 13.8M/22.1M [00:40<00:14, 565kB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "optimizer.bin:  68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã       | 15.1M/22.1M [00:37<00:03, 2.07MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "scheduler.bin: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1.00k/1.00k [00:00<00:00, 2.13kB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "optimizer.bin:  70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà       | 15.5M/22.1M [00:37<00:04, 1.45MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "optimizer.bin:   0%|                                | 0.00/22.1M [00:00<?, ?B/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "pytorch_lora_weights.safetensors:  25%|‚ñà‚ñé   | 2.75M/10.9M [00:04<00:13, 627kB/s]\u001b[A\u001b[A\u001b[A\n",
      "optimizer.bin:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ        | 14.7M/22.1M [00:42<00:15, 491kB/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "optimizer.bin:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè      | 15.8M/22.1M [00:38<00:06, 961kB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "optimizer.bin:   3%|‚ñä                        | 672k/22.1M [00:00<00:29, 727kB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "optimizer.bin:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé        | 14.1M/22.1M [00:42<00:24, 324kB/s]\u001b[A\u001b[A\n",
      "optimizer.bin:  68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç       | 15.1M/22.1M [00:43<00:17, 410kB/s]\u001b[A\n",
      "\n",
      "\n",
      "pytorch_lora_weights.safetensors:  28%|‚ñà‚ñç   | 3.08M/10.9M [00:06<00:22, 350kB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "optimizer.bin:   4%|‚ñà                        | 934k/22.1M [00:02<00:53, 398kB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "optimizer.bin:  70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä       | 15.5M/22.1M [00:44<00:15, 437kB/s]\u001b[A\n",
      "\n",
      "optimizer.bin:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã        | 14.4M/22.1M [00:43<00:25, 306kB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "optimizer.bin:   5%|‚ñà‚ñé                      | 1.20M/22.1M [00:02<00:50, 414kB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "optimizer.bin:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè      | 15.9M/22.1M [00:44<00:11, 532kB/s]\u001b[A\n",
      "\n",
      "\n",
      "pytorch_lora_weights.safetensors:  31%|‚ñà‚ñå   | 3.41M/10.9M [00:07<00:21, 349kB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "optimizer.bin:   7%|‚ñà‚ñå                      | 1.46M/22.1M [00:03<00:46, 447kB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "optimizer.bin:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ        | 14.7M/22.1M [00:44<00:21, 346kB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "optimizer.bin:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç      | 16.0M/22.1M [00:41<00:21, 290kB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "optimizer.bin:   8%|‚ñà‚ñä                      | 1.72M/22.1M [00:03<00:36, 562kB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "pytorch_lora_weights.safetensors:  34%|‚ñà‚ñã   | 3.72M/10.9M [00:07<00:17, 411kB/s]\u001b[A\u001b[A\u001b[A\n",
      "optimizer.bin:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç      | 16.0M/22.1M [00:45<00:14, 433kB/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "optimizer.bin:   9%|‚ñà‚ñà‚ñè                     | 1.97M/22.1M [00:03<00:30, 658kB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "optimizer.bin:  77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå     | 17.1M/22.1M [00:45<00:05, 977kB/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "optimizer.bin:  77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå     | 17.1M/22.1M [00:41<00:08, 622kB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "optimizer.bin:  10%|‚ñà‚ñà‚ñç                     | 2.23M/22.1M [00:03<00:25, 787kB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "pytorch_lora_weights.safetensors:  37%|‚ñà‚ñä   | 4.05M/10.9M [00:07<00:14, 488kB/s]\u001b[A\u001b[A\u001b[A\n",
      "optimizer.bin:  79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 17.5M/22.1M [00:45<00:04, 1.05MB/s]\u001b[A\n",
      "\n",
      "optimizer.bin:  68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé       | 15.1M/22.1M [00:44<00:19, 356kB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "optimizer.bin:  79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ     | 17.5M/22.1M [00:42<00:07, 663kB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "optimizer.bin:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 17.9M/22.1M [00:45<00:03, 1.17MB/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "optimizer.bin:  11%|‚ñà‚ñà‚ñã                     | 2.49M/22.1M [00:04<00:26, 732kB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "optimizer.bin:  83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 18.3M/22.1M [00:46<00:02, 1.35MB/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "optimizer.bin:  12%|‚ñà‚ñà‚ñâ                     | 2.75M/22.1M [00:04<00:26, 729kB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "pytorch_lora_weights.safetensors:  40%|‚ñà‚ñà   | 4.37M/10.9M [00:08<00:13, 473kB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "optimizer.bin:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 17.9M/22.1M [00:42<00:06, 683kB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "optimizer.bin:  85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 18.7M/22.1M [00:46<00:03, 972kB/s]\u001b[A\n",
      "\n",
      "optimizer.bin:  70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã       | 15.4M/22.1M [00:45<00:19, 340kB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "optimizer.bin:  14%|‚ñà‚ñà‚ñà‚ñé                    | 3.01M/22.1M [00:05<00:32, 583kB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "pytorch_lora_weights.safetensors:  43%|‚ñà‚ñà‚ñè  | 4.70M/10.9M [00:09<00:14, 443kB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "optimizer.bin:  83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 18.3M/22.1M [00:43<00:05, 643kB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "optimizer.bin:  86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 19.1M/22.1M [00:47<00:03, 978kB/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "optimizer.bin:  15%|‚ñà‚ñà‚ñà‚ñå                    | 3.26M/22.1M [00:05<00:32, 572kB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "optimizer.bin:  85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 18.7M/22.1M [00:43<00:04, 684kB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "optimizer.bin:  71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà       | 15.7M/22.1M [00:46<00:17, 363kB/s]\u001b[A\u001b[A\n",
      "optimizer.bin:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 19.5M/22.1M [00:47<00:02, 922kB/s]\u001b[A\n",
      "\n",
      "\n",
      "pytorch_lora_weights.safetensors:  46%|‚ñà‚ñà‚ñé  | 5.01M/10.9M [00:10<00:12, 464kB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "optimizer.bin:  16%|‚ñà‚ñà‚ñà‚ñä                    | 3.52M/22.1M [00:06<00:32, 577kB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "optimizer.bin:  86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 19.1M/22.1M [00:44<00:03, 807kB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "optimizer.bin:  90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 19.9M/22.1M [00:47<00:02, 1.07MB/s]\u001b[A\n",
      "\n",
      "\n",
      "pytorch_lora_weights.safetensors:  48%|‚ñà‚ñà‚ñç  | 5.28M/10.9M [00:10<00:10, 553kB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "optimizer.bin:  17%|‚ñà‚ñà‚ñà‚ñà                    | 3.78M/22.1M [00:06<00:28, 647kB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "optimizer.bin:  92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 20.3M/22.1M [00:48<00:01, 1.13MB/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "optimizer.bin:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 19.5M/22.1M [00:44<00:02, 890kB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "pytorch_lora_weights.safetensors:  51%|‚ñà‚ñà‚ñå  | 5.60M/10.9M [00:10<00:08, 620kB/s]\u001b[A\u001b[A\u001b[A\n",
      "optimizer.bin:  93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 20.7M/22.1M [00:48<00:01, 1.24MB/s]\u001b[A\n",
      "\n",
      "\n",
      "pytorch_lora_weights.safetensors:  54%|‚ñà‚ñà‚ñã  | 5.87M/10.9M [00:10<00:06, 726kB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "optimizer.bin:  18%|‚ñà‚ñà‚ñà‚ñà‚ñç                   | 4.05M/22.1M [00:06<00:28, 633kB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "optimizer.bin:  90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 19.9M/22.1M [00:44<00:02, 932kB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "optimizer.bin:  95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 21.1M/22.1M [00:48<00:00, 1.38MB/s]\u001b[A\n",
      "\n",
      "\n",
      "pytorch_lora_weights.safetensors:  57%|‚ñà‚ñà‚ñä  | 6.18M/10.9M [00:11<00:05, 850kB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "optimizer.bin:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç      | 16.0M/22.1M [00:47<00:18, 330kB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "optimizer.bin:  19%|‚ñà‚ñà‚ñà‚ñà‚ñã                   | 4.31M/22.1M [00:07<00:25, 701kB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "optimizer.bin:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 21.5M/22.1M [00:48<00:00, 1.47MB/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "optimizer.bin:  92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 20.2M/22.1M [00:45<00:01, 992kB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "pytorch_lora_weights.safetensors:  60%|‚ñà‚ñà‚ñâ  | 6.50M/10.9M [00:11<00:04, 972kB/s]\u001b[A\u001b[A\u001b[A\n",
      "optimizer.bin:  99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 21.8M/22.1M [00:49<00:00, 1.58MB/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "optimizer.bin:  21%|‚ñà‚ñà‚ñà‚ñà‚ñâ                   | 4.55M/22.1M [00:07<00:24, 724kB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "optimizer.bin:  93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 20.6M/22.1M [00:45<00:01, 1.07MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "pytorch_lora_weights.safetensors:  63%|‚ñà‚ñà‚ñå | 6.83M/10.9M [00:11<00:03, 1.05MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "optimizer.bin:  76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé     | 16.9M/22.1M [00:48<00:09, 577kB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "optimizer.bin:  22%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                  | 4.82M/22.1M [00:07<00:26, 644kB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "pytorch_lora_weights.safetensors:  66%|‚ñà‚ñà‚ñà‚ñé | 7.16M/10.9M [00:12<00:04, 882kB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "optimizer.bin:  78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã     | 17.2M/22.1M [00:48<00:07, 620kB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "optimizer.bin:  95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 21.0M/22.1M [00:45<00:01, 922kB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "pytorch_lora_weights.safetensors:  68%|‚ñà‚ñà‚ñà‚ñç | 7.41M/10.9M [00:12<00:04, 864kB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "optimizer.bin: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 22.1M/22.1M [00:50<00:00, 441kB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "optimizer.bin:  79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà     | 17.5M/22.1M [00:49<00:07, 631kB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "pytorch_lora_weights.safetensors:  70%|‚ñà‚ñà‚ñà‚ñå | 7.67M/10.9M [00:12<00:04, 714kB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "optimizer.bin:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 21.4M/22.1M [00:46<00:00, 715kB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "optimizer.bin:  24%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                  | 5.34M/22.1M [00:08<00:29, 566kB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "pytorch_lora_weights.safetensors:  73%|‚ñà‚ñà‚ñà‚ñã | 8.00M/10.9M [00:13<00:03, 912kB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "optimizer.bin:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 17.9M/22.1M [00:49<00:06, 638kB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "optimizer.bin:  25%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                  | 5.60M/22.1M [00:09<00:25, 650kB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "optimizer.bin:  99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 21.9M/22.1M [00:47<00:00, 839kB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "pytorch_lora_weights.safetensors:  76%|‚ñà‚ñà‚ñà | 8.32M/10.9M [00:13<00:02, 1.01MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Upload 26 LFS files:   4%|‚ñâ                      | 1/26 [00:50<21:13, 50.93s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "pytorch_lora_weights.safetensors:   0%|             | 0.00/10.9M [00:00<?, ?B/s]\u001b[A\n",
      "\n",
      "optimizer.bin:  82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 18.2M/22.1M [00:50<00:05, 719kB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "optimizer.bin:  26%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                 | 5.85M/22.1M [00:09<00:21, 750kB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "pytorch_lora_weights.safetensors:  79%|‚ñà‚ñà‚ñà‚ñè| 8.63M/10.9M [00:13<00:01, 1.14MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "optimizer.bin:  28%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                 | 6.11M/22.1M [00:09<00:18, 845kB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "optimizer.bin:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 18.6M/22.1M [00:50<00:04, 832kB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "pytorch_lora_weights.safetensors:  82%|‚ñà‚ñà‚ñà‚ñé| 8.96M/10.9M [00:13<00:01, 1.29MB/s]\u001b[A\u001b[A\u001b[A\n",
      "pytorch_lora_weights.safetensors:  10%|‚ñç   | 1.13M/10.9M [00:00<00:03, 2.55MB/s]\u001b[A\n",
      "\n",
      "\n",
      "pytorch_lora_weights.safetensors:  85%|‚ñà‚ñà‚ñà‚ñç| 9.29M/10.9M [00:13<00:01, 1.41MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "optimizer.bin:  85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 18.9M/22.1M [00:50<00:03, 889kB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "optimizer.bin:  29%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                 | 6.37M/22.1M [00:09<00:18, 848kB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "pytorch_lora_weights.safetensors:  88%|‚ñà‚ñà‚ñà‚ñå| 9.62M/10.9M [00:14<00:00, 1.43MB/s]\u001b[A\u001b[A\u001b[A\n",
      "optimizer.bin: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 22.1M/22.1M [00:48<00:00, 460kB/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "optimizer.bin:  30%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                | 6.64M/22.1M [00:10<00:18, 826kB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "optimizer.bin:  87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 19.2M/22.1M [00:51<00:03, 885kB/s]\u001b[A\u001b[A\n",
      "pytorch_lora_weights.safetensors:  19%|‚ñä   | 2.05M/10.9M [00:01<00:04, 1.94MB/s]\u001b[A\n",
      "\n",
      "\n",
      "pytorch_lora_weights.safetensors:  91%|‚ñà‚ñà‚ñà‚ñã| 9.93M/10.9M [00:14<00:00, 1.34MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "random_states_0.pkl:   0%|                          | 0.00/14.3k [00:00<?, ?B/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "pytorch_lora_weights.safetensors:  22%|‚ñâ   | 2.42M/10.9M [00:01<00:05, 1.69MB/s]\u001b[A\n",
      "\n",
      "\n",
      "pytorch_lora_weights.safetensors:  94%|‚ñà‚ñà‚ñà‚ñä| 10.3M/10.9M [00:14<00:00, 1.11MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "optimizer.bin:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 19.6M/22.1M [00:51<00:03, 834kB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "random_states_0.pkl: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 14.3k/14.3k [00:00<00:00, 49.9kB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch_lora_weights.safetensors:  26%|‚ñà   | 2.82M/10.9M [00:01<00:05, 1.40MB/s]\u001b[A\n",
      "\n",
      "\n",
      "pytorch_lora_weights.safetensors:  97%|‚ñà‚ñà‚ñà‚ñâ| 10.6M/10.9M [00:15<00:00, 1.09MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "optimizer.bin:  90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 19.9M/22.1M [00:51<00:02, 853kB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "optimizer.bin:  33%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                | 7.21M/22.1M [00:11<00:19, 777kB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "pytorch_lora_weights.safetensors:  30%|‚ñà‚ñè  | 3.28M/10.9M [00:01<00:04, 1.58MB/s]\u001b[A\n",
      "\n",
      "optimizer.bin:  91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 20.2M/22.1M [00:51<00:01, 1.02MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "pytorch_lora_weights.safetensors: 100%|‚ñà‚ñà‚ñà‚ñà| 10.9M/10.9M [00:15<00:00, 1.17MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "optimizer.bin:  34%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                | 7.47M/22.1M [00:11<00:16, 905kB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "scaler.pt:   0%|                                      | 0.00/988 [00:00<?, ?B/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "pytorch_lora_weights.safetensors:  33%|‚ñà‚ñé  | 3.65M/10.9M [00:02<00:04, 1.60MB/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "optimizer.bin:  35%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà               | 7.73M/22.1M [00:11<00:14, 1.00MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "scaler.pt: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 988/988 [00:00<00:00, 5.15kB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_lora_weights.safetensors:  37%|‚ñà‚ñç  | 4.05M/10.9M [00:02<00:03, 1.79MB/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "optimizer.bin:  36%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé              | 8.00M/22.1M [00:11<00:13, 1.08MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch_lora_weights.safetensors: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà| 10.9M/10.9M [00:15<00:00, 690kB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_lora_weights.safetensors:  41%|‚ñà‚ñã  | 4.44M/10.9M [00:02<00:03, 2.00MB/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "optimizer.bin:  37%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå              | 8.24M/22.1M [00:11<00:11, 1.20MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "optimizer.bin:  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 21.2M/22.1M [00:52<00:00, 1.47MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "scheduler.bin:   0%|                                | 0.00/1.00k [00:00<?, ?B/s]\u001b[A\u001b[A\u001b[A\n",
      "pytorch_lora_weights.safetensors:  44%|‚ñà‚ñä  | 4.82M/10.9M [00:02<00:02, 2.12MB/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "scheduler.bin: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1.00k/1.00k [00:00<00:00, 5.38kB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "optimizer.bin:  38%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè              | 8.50M/22.1M [00:12<00:15, 904kB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "optimizer.bin:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 21.6M/22.1M [00:53<00:00, 1.03MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "pytorch_lora_weights.safetensors:   0%|             | 0.00/10.9M [00:00<?, ?B/s]\u001b[A\u001b[A\u001b[A\n",
      "pytorch_lora_weights.safetensors:  48%|‚ñà‚ñâ  | 5.21M/10.9M [00:03<00:04, 1.18MB/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "optimizer.bin:  40%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå              | 8.77M/22.1M [00:12<00:18, 728kB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "optimizer.bin:   4%|‚ñâ                        | 819k/22.1M [00:00<00:21, 978kB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "pytorch_lora_weights.safetensors:  51%|‚ñà‚ñà  | 5.60M/10.9M [00:03<00:04, 1.09MB/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "optimizer.bin:  41%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä              | 9.03M/22.1M [00:13<00:16, 771kB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "optimizer.bin:  99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 21.9M/22.1M [00:53<00:00, 794kB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "pytorch_lora_weights.safetensors:  10%|‚ñç   | 1.06M/10.9M [00:00<00:06, 1.46MB/s]\u001b[A\u001b[A\u001b[A\n",
      "pytorch_lora_weights.safetensors:  55%|‚ñà‚ñà‚ñè | 5.98M/10.9M [00:03<00:04, 1.17MB/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "optimizer.bin:   5%|‚ñà‚ñè                      | 1.13M/22.1M [00:01<00:24, 871kB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "optimizer.bin:  42%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà              | 9.29M/22.1M [00:13<00:14, 871kB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "optimizer.bin:  43%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé             | 9.54M/22.1M [00:13<00:13, 911kB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "pytorch_lora_weights.safetensors:  13%|‚ñå   | 1.46M/10.9M [00:01<00:07, 1.34MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "optimizer.bin:   7%|‚ñà‚ñå                      | 1.46M/22.1M [00:01<00:24, 832kB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "pytorch_lora_weights.safetensors:  58%|‚ñà‚ñà‚ñé | 6.37M/10.9M [00:04<00:04, 1.01MB/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "optimizer.bin:  44%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã             | 9.80M/22.1M [00:13<00:13, 918kB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "optimizer.bin:  46%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå            | 10.1M/22.1M [00:14<00:11, 1.02MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "optimizer.bin:   8%|‚ñà‚ñâ                      | 1.79M/22.1M [00:02<00:24, 844kB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "optimizer.bin: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 22.1M/22.1M [00:55<00:00, 401kB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "optimizer.bin:  47%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä            | 10.5M/22.1M [00:14<00:11, 1.00MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "pytorch_lora_weights.safetensors:  62%|‚ñà‚ñà‚ñà  | 6.77M/10.9M [00:05<00:06, 653kB/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Upload 26 LFS files:  23%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                 | 6/26 [00:56<02:25,  7.28s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "random_states_0.pkl: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 14.3k/14.3k [00:00<00:00, 41.2kB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "optimizer.bin:  10%|‚ñà‚ñà‚ñé                     | 2.11M/22.1M [00:03<00:44, 446kB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "pytorch_lora_weights.safetensors:  20%|‚ñà    | 2.23M/10.9M [00:03<00:19, 437kB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "optimizer.bin:  49%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä            | 10.8M/22.1M [00:16<00:25, 440kB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "scaler.pt:   0%|                                      | 0.00/988 [00:00<?, ?B/s]\u001b[A\u001b[A\n",
      "scaler.pt: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 988/988 [00:00<00:00, 5.87kB/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "optimizer.bin:  11%|‚ñà‚ñà‚ñã                     | 2.42M/22.1M [00:04<00:52, 378kB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "scheduler.bin: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1.00k/1.00k [00:00<00:00, 2.07kB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "pytorch_lora_weights.safetensors:  24%|‚ñà‚ñè   | 2.62M/10.9M [00:05<00:22, 367kB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "optimizer.bin:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà            | 11.2M/22.1M [00:17<00:31, 344kB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "pytorch_lora_weights.safetensors:  69%|‚ñà‚ñà‚ñà‚ñç | 7.54M/10.9M [00:08<00:08, 393kB/s]\u001b[A\n",
      "\n",
      "pytorch_lora_weights.safetensors:   0%|             | 0.00/10.9M [00:00<?, ?B/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_lora_weights.safetensors:   8%|‚ñç    | 868k/10.9M [00:00<00:03, 2.77MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "optimizer.bin:  12%|‚ñà‚ñà‚ñâ                     | 2.75M/22.1M [00:06<01:03, 304kB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "optimizer.bin:  52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç           | 11.5M/22.1M [00:19<00:36, 292kB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch_lora_weights.safetensors:  11%|‚ñå    | 1.20M/10.9M [00:01<00:14, 661kB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "pytorch_lora_weights.safetensors:  28%|‚ñà‚ñç   | 3.01M/10.9M [00:07<00:28, 274kB/s]\u001b[A\u001b[A\u001b[A\n",
      "pytorch_lora_weights.safetensors:  73%|‚ñà‚ñà‚ñà‚ñã | 7.93M/10.9M [00:10<00:11, 268kB/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "optimizer.bin:  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä           | 11.8M/22.1M [00:20<00:36, 286kB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "optimizer.bin:  14%|‚ñà‚ñà‚ñà‚ñé                    | 3.08M/22.1M [00:08<01:21, 233kB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch_lora_weights.safetensors:  14%|‚ñã    | 1.52M/10.9M [00:02<00:22, 419kB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "pytorch_lora_weights.safetensors:  31%|‚ñà‚ñå   | 3.39M/10.9M [00:08<00:26, 282kB/s]\u001b[A\u001b[A\u001b[A\n",
      "pytorch_lora_weights.safetensors:  76%|‚ñà‚ñà‚ñà‚ñä | 8.32M/10.9M [00:11<00:08, 298kB/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "optimizer.bin:  55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè          | 12.1M/22.1M [00:21<00:32, 306kB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch_lora_weights.safetensors:  17%|‚ñä    | 1.85M/10.9M [00:03<00:20, 445kB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "optimizer.bin:  15%|‚ñà‚ñà‚ñà‚ñã                    | 3.41M/22.1M [00:10<01:32, 203kB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch_lora_weights.safetensors:  20%|‚ñâ    | 2.16M/10.9M [00:04<00:24, 351kB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "pytorch_lora_weights.safetensors:  35%|‚ñà‚ñã   | 3.78M/10.9M [00:10<00:26, 268kB/s]\u001b[A\u001b[A\u001b[A\n",
      "pytorch_lora_weights.safetensors:  80%|‚ñà‚ñà‚ñà‚ñâ | 8.70M/10.9M [00:15<00:10, 202kB/s]\u001b[A\n",
      "\n",
      "pytorch_lora_weights.safetensors:  23%|‚ñà‚ñè   | 2.49M/10.9M [00:07<00:35, 240kB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "optimizer.bin:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå          | 12.5M/22.1M [00:24<00:55, 176kB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "optimizer.bin:  17%|‚ñà‚ñà‚ñà‚ñà                    | 3.72M/22.1M [00:12<01:47, 171kB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch_lora_weights.safetensors:  26%|‚ñà‚ñé   | 2.82M/10.9M [00:07<00:29, 273kB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "pytorch_lora_weights.safetensors:  38%|‚ñà‚ñâ   | 4.18M/10.9M [00:13<00:33, 200kB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "optimizer.bin:  18%|‚ñà‚ñà‚ñà‚ñà‚ñç                   | 4.05M/22.1M [00:14<01:32, 194kB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "pytorch_lora_weights.safetensors:  83%|‚ñà‚ñà‚ñà‚ñà‚ñè| 9.09M/10.9M [00:17<00:09, 198kB/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "optimizer.bin:  58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä          | 12.8M/22.1M [00:26<00:54, 172kB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "pytorch_lora_weights.safetensors:  42%|‚ñà‚ñà   | 4.55M/10.9M [00:14<00:28, 224kB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch_lora_weights.safetensors:  29%|‚ñà‚ñç   | 3.15M/10.9M [00:09<00:29, 260kB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "optimizer.bin:  20%|‚ñà‚ñà‚ñà‚ñà‚ñã                   | 4.37M/22.1M [00:15<01:22, 215kB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "pytorch_lora_weights.safetensors:  87%|‚ñà‚ñà‚ñà‚ñà‚ñé| 9.49M/10.9M [00:18<00:06, 237kB/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "optimizer.bin:  59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè         | 13.1M/22.1M [00:27<00:43, 209kB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "optimizer.bin:  21%|‚ñà‚ñà‚ñà‚ñà‚ñà                   | 4.64M/22.1M [00:15<01:10, 247kB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch_lora_weights.safetensors:  32%|‚ñà‚ñå   | 3.46M/10.9M [00:10<00:26, 277kB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "pytorch_lora_weights.safetensors:  45%|‚ñà‚ñà‚ñé  | 4.95M/10.9M [00:15<00:23, 254kB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "optimizer.bin:  61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå         | 13.4M/22.1M [00:28<00:33, 259kB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "pytorch_lora_weights.safetensors:  90%|‚ñà‚ñà‚ñà‚ñà‚ñå| 9.86M/10.9M [00:18<00:03, 277kB/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "optimizer.bin:  62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ         | 13.7M/22.1M [00:28<00:26, 318kB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch_lora_weights.safetensors:  35%|‚ñà‚ñã   | 3.78M/10.9M [00:11<00:22, 315kB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "pytorch_lora_weights.safetensors:  49%|‚ñà‚ñà‚ñç  | 5.34M/10.9M [00:16<00:18, 295kB/s]\u001b[A\u001b[A\u001b[A\n",
      "pytorch_lora_weights.safetensors:  94%|‚ñà‚ñà‚ñà‚ñà‚ñã| 10.3M/10.9M [00:19<00:01, 330kB/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "optimizer.bin:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé        | 14.1M/22.1M [00:29<00:20, 388kB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "optimizer.bin:  22%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                  | 4.95M/22.1M [00:17<01:07, 253kB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch_lora_weights.safetensors:  38%|‚ñà‚ñâ   | 4.11M/10.9M [00:13<00:28, 237kB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "optimizer.bin:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå        | 14.4M/22.1M [00:30<00:27, 281kB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "pytorch_lora_weights.safetensors:  52%|‚ñà‚ñà‚ñå  | 5.72M/10.9M [00:18<00:20, 248kB/s]\u001b[A\u001b[A\u001b[A\n",
      "pytorch_lora_weights.safetensors:  98%|‚ñà‚ñà‚ñà‚ñà‚ñâ| 10.6M/10.9M [00:21<00:00, 270kB/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "optimizer.bin:  24%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                  | 5.21M/22.1M [00:19<01:24, 200kB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "optimizer.bin:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ        | 14.7M/22.1M [00:31<00:20, 357kB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch_lora_weights.safetensors:  41%|‚ñà‚ñà   | 4.42M/10.9M [00:14<00:24, 266kB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "optimizer.bin:  25%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                  | 5.54M/22.1M [00:19<01:05, 254kB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "optimizer.bin:  68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé       | 15.0M/22.1M [00:31<00:18, 383kB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "pytorch_lora_weights.safetensors:  56%|‚ñà‚ñà‚ñä  | 6.11M/10.9M [00:20<00:19, 243kB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "optimizer.bin:  26%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                 | 5.80M/22.1M [00:20<01:05, 248kB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch_lora_weights.safetensors:  44%|‚ñà‚ñà‚ñè  | 4.75M/10.9M [00:15<00:23, 262kB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "optimizer.bin:  69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã       | 15.4M/22.1M [00:34<00:25, 260kB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "optimizer.bin:  28%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                 | 6.11M/22.1M [00:22<01:06, 241kB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch_lora_weights.safetensors:  47%|‚ñà‚ñà‚ñé  | 5.08M/10.9M [00:16<00:22, 257kB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "optimizer.bin:  29%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                 | 6.44M/22.1M [00:22<00:56, 279kB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch_lora_weights.safetensors:  50%|‚ñà‚ñà‚ñç  | 5.41M/10.9M [00:17<00:18, 295kB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "pytorch_lora_weights.safetensors:  60%|‚ñà‚ñà‚ñâ  | 6.50M/10.9M [00:22<00:21, 207kB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch_lora_weights.safetensors: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà| 10.9M/10.9M [00:26<00:00, 416kB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "optimizer.bin:  31%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                | 6.77M/22.1M [00:23<00:47, 323kB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "optimizer.bin:  71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà       | 15.7M/22.1M [00:35<00:28, 226kB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch_lora_weights.safetensors:  55%|‚ñà‚ñà‚ñä  | 6.05M/10.9M [00:18<00:14, 345kB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "optimizer.bin:  32%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                | 7.09M/22.1M [00:24<00:44, 337kB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch_lora_weights.safetensors:  58%|‚ñà‚ñà‚ñâ  | 6.37M/10.9M [00:19<00:11, 385kB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "pytorch_lora_weights.safetensors:  63%|‚ñà‚ñà‚ñà‚ñè | 6.88M/10.9M [00:24<00:19, 205kB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "optimizer.bin:  33%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                | 7.41M/22.1M [00:25<00:46, 319kB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch_lora_weights.safetensors:  61%|‚ñà‚ñà‚ñà  | 6.70M/10.9M [00:20<00:10, 411kB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "optimizer.bin:  35%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç               | 7.73M/22.1M [00:25<00:34, 422kB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "optimizer.bin:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé      | 16.0M/22.1M [00:37<00:29, 204kB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "pytorch_lora_weights.safetensors:  67%|‚ñà‚ñà‚ñà‚ñé | 7.27M/10.9M [00:25<00:14, 249kB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch_lora_weights.safetensors:  64%|‚ñà‚ñà‚ñà‚ñè | 7.01M/10.9M [00:20<00:08, 477kB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "optimizer.bin:  36%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã               | 8.06M/22.1M [00:26<00:30, 468kB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "pytorch_lora_weights.safetensors:  70%|‚ñà‚ñà‚ñà‚ñå | 7.67M/10.9M [00:26<00:10, 306kB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch_lora_weights.safetensors:  67%|‚ñà‚ñà‚ñà‚ñé | 7.34M/10.9M [00:20<00:06, 557kB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "optimizer.bin:  38%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà               | 8.39M/22.1M [00:26<00:25, 531kB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch_lora_weights.safetensors:  70%|‚ñà‚ñà‚ñà‚ñå | 7.67M/10.9M [00:21<00:05, 615kB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "optimizer.bin:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç      | 16.0M/22.1M [00:38<00:37, 163kB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "pytorch_lora_weights.safetensors:  74%|‚ñà‚ñà‚ñà‚ñã | 8.06M/10.9M [00:26<00:08, 337kB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch_lora_weights.safetensors:  73%|‚ñà‚ñà‚ñà‚ñã | 8.00M/10.9M [00:21<00:04, 603kB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "optimizer.bin:  39%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé              | 8.63M/22.1M [00:27<00:28, 465kB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "optimizer.bin:  76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé     | 16.9M/22.1M [00:40<00:16, 314kB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "optimizer.bin:  41%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã              | 8.96M/22.1M [00:28<00:35, 374kB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch_lora_weights.safetensors:  76%|‚ñà‚ñà‚ñà‚ñä | 8.31M/10.9M [00:23<00:06, 391kB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "pytorch_lora_weights.safetensors:  77%|‚ñà‚ñà‚ñà‚ñä | 8.44M/10.9M [00:28<00:08, 282kB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "optimizer.bin:  78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã     | 17.2M/22.1M [00:41<00:16, 295kB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "pytorch_lora_weights.safetensors:  81%|‚ñà‚ñà‚ñà‚ñà | 8.83M/10.9M [00:30<00:08, 256kB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "optimizer.bin:  79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà     | 17.5M/22.1M [00:43<00:20, 222kB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "optimizer.bin:  42%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà              | 9.29M/22.1M [00:31<01:01, 209kB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch_lora_weights.safetensors:  79%|‚ñà‚ñà‚ñà‚ñâ | 8.63M/10.9M [00:26<00:10, 216kB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "pytorch_lora_weights.safetensors:  85%|‚ñà‚ñà‚ñà‚ñà‚ñè| 9.22M/10.9M [00:33<00:08, 207kB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "optimizer.bin:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 17.9M/22.1M [00:47<00:27, 157kB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch_lora_weights.safetensors:  82%|‚ñà‚ñà‚ñà‚ñà | 8.96M/10.9M [00:29<00:12, 155kB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "optimizer.bin:  43%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç             | 9.62M/22.1M [00:35<01:26, 144kB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "pytorch_lora_weights.safetensors:  88%|‚ñà‚ñà‚ñà‚ñà‚ñç| 9.60M/10.9M [00:35<00:06, 204kB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch_lora_weights.safetensors:  85%|‚ñà‚ñà‚ñà‚ñà‚ñé| 9.29M/10.9M [00:30<00:07, 205kB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "optimizer.bin:  45%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã             | 9.86M/22.1M [00:35<01:08, 179kB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "optimizer.bin:  82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 18.2M/22.1M [00:48<00:20, 196kB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch_lora_weights.safetensors:  88%|‚ñà‚ñà‚ñà‚ñà‚ñç| 9.60M/10.9M [00:30<00:04, 273kB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "pytorch_lora_weights.safetensors:  92%|‚ñà‚ñà‚ñà‚ñà‚ñå| 9.99M/10.9M [00:35<00:03, 266kB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "optimizer.bin:  46%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà             | 10.2M/22.1M [00:36<00:48, 246kB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch_lora_weights.safetensors:  91%|‚ñà‚ñà‚ñà‚ñà‚ñå| 9.93M/10.9M [00:30<00:02, 346kB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "pytorch_lora_weights.safetensors:  95%|‚ñà‚ñà‚ñà‚ñà‚ñä| 10.4M/10.9M [00:36<00:01, 344kB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "optimizer.bin:  48%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç            | 10.5M/22.1M [00:36<00:35, 326kB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "optimizer.bin:  49%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã            | 10.8M/22.1M [00:36<00:27, 408kB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "optimizer.bin:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 18.5M/22.1M [00:48<00:15, 235kB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch_lora_weights.safetensors:  94%|‚ñà‚ñà‚ñà‚ñà‚ñã| 10.3M/10.9M [00:31<00:01, 433kB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "pytorch_lora_weights.safetensors:  99%|‚ñà‚ñà‚ñà‚ñà‚ñâ| 10.8M/10.9M [00:36<00:00, 415kB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "optimizer.bin:  85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 18.8M/22.1M [00:49<00:10, 302kB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch_lora_weights.safetensors:  97%|‚ñà‚ñà‚ñà‚ñà‚ñä| 10.6M/10.9M [00:31<00:00, 534kB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "optimizer.bin:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà            | 11.1M/22.1M [00:37<00:22, 488kB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "optimizer.bin:  87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 19.1M/22.1M [00:49<00:07, 385kB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pytorch_lora_weights.safetensors: 100%|‚ñà‚ñà‚ñà‚ñà‚ñâ| 10.9M/10.9M [00:31<00:00, 621kB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "optimizer.bin:  52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç           | 11.4M/22.1M [00:37<00:18, 577kB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "optimizer.bin:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 19.5M/22.1M [00:49<00:05, 498kB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "optimizer.bin:  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã           | 11.7M/22.1M [00:37<00:16, 640kB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "pytorch_lora_weights.safetensors: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà| 10.9M/10.9M [00:32<00:00, 334kB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "optimizer.bin:  55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà           | 12.1M/22.1M [00:38<00:15, 666kB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "pytorch_lora_weights.safetensors: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà| 10.9M/10.9M [00:37<00:00, 288kB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "optimizer.bin:  92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 20.4M/22.1M [00:50<00:02, 735kB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "optimizer.bin:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç          | 12.4M/22.1M [00:38<00:13, 748kB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "optimizer.bin:  94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 20.8M/22.1M [00:50<00:01, 808kB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "optimizer.bin:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä          | 12.7M/22.1M [00:39<00:12, 759kB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "optimizer.bin:  95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 21.1M/22.1M [00:51<00:01, 845kB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "optimizer.bin:  59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà          | 13.0M/22.1M [00:39<00:12, 741kB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "optimizer.bin:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 21.4M/22.1M [00:51<00:00, 870kB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "optimizer.bin:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç         | 13.3M/22.1M [00:39<00:10, 846kB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "optimizer.bin:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 21.7M/22.1M [00:51<00:00, 941kB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "optimizer.bin:  62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä         | 13.6M/22.1M [00:39<00:09, 900kB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "optimizer.bin: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 22.1M/22.1M [00:52<00:00, 972kB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "optimizer.bin:  63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç        | 13.9M/22.1M [00:40<00:07, 1.02MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "optimizer.bin:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä        | 14.3M/22.1M [00:40<00:07, 1.10MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "optimizer.bin: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 22.1M/22.1M [00:52<00:00, 418kB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "optimizer.bin:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå       | 14.9M/22.1M [00:40<00:06, 1.19MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "optimizer.bin:  69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä       | 15.2M/22.1M [00:41<00:05, 1.27MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Upload 26 LFS files:  62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå        | 16/26 [01:34<00:47,  4.79s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "optimizer.bin:  70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè      | 15.6M/22.1M [00:41<00:05, 1.27MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "optimizer.bin:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå      | 15.9M/22.1M [00:41<00:05, 1.21MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "optimizer.bin:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç      | 16.0M/22.1M [00:42<00:08, 696kB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "optimizer.bin:  76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç     | 16.8M/22.1M [00:42<00:04, 1.17MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "optimizer.bin:  78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä     | 17.1M/22.1M [00:42<00:03, 1.29MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "optimizer.bin:  79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 17.5M/22.1M [00:43<00:03, 1.38MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "optimizer.bin:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 17.8M/22.1M [00:43<00:03, 1.42MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "optimizer.bin:  82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 18.1M/22.1M [00:43<00:02, 1.47MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "optimizer.bin:  83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 18.4M/22.1M [00:43<00:02, 1.58MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "optimizer.bin:  85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 18.8M/22.1M [00:43<00:01, 1.72MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "optimizer.bin:  86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 19.1M/22.1M [00:43<00:01, 1.74MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "optimizer.bin:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 19.4M/22.1M [00:44<00:01, 1.81MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "optimizer.bin:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 19.7M/22.1M [00:44<00:01, 1.90MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "optimizer.bin:  91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 20.1M/22.1M [00:44<00:01, 2.05MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "optimizer.bin:  92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 20.4M/22.1M [00:44<00:00, 1.86MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "optimizer.bin:  94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 20.7M/22.1M [00:44<00:00, 1.81MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "optimizer.bin:  95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 21.0M/22.1M [00:44<00:00, 1.78MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "optimizer.bin:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 21.4M/22.1M [00:45<00:00, 1.62MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "optimizer.bin:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 21.7M/22.1M [00:45<00:00, 1.64MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "optimizer.bin: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 22.1M/22.1M [00:46<00:00, 477kB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Upload 26 LFS files: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 26/26 [01:40<00:00,  3.87s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: W&B sync reduced upload amount by 9.0%             \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: loss ‚ñÅ‚ñÉ‚ñÑ‚ñÇ‚ñÅ‚ñÅ‚ñà‚ñá‚ñÉ‚ñÅ‚ñÖ‚ñÖ‚ñÅ‚ñá‚ñÇ‚ñÑ‚ñÅ‚ñÅ‚ñÑ‚ñÇ‚ñÅ‚ñÑ‚ñá‚ñÅ‚ñÜ‚ñÑ‚ñÅ‚ñÇ‚ñÑ‚ñÅ‚ñÅ‚ñÅ‚ñá‚ñÖ‚ñÇ‚ñÇ‚ñá‚ñÅ‚ñÇ‚ñÇ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   lr ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñá‚ñá‚ñá‚ñá‚ñá‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: loss 0.37806\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   lr 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: üöÄ View run \u001b[33musual-plant-7\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/philipp-zettl/dreambooth-lora-sd-xl/runs/gs0vw9iu\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ‚≠êÔ∏è View project at: \u001b[34m\u001b[4mhttps://wandb.ai/philipp-zettl/dreambooth-lora-sd-xl\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 2 artifact file(s) and 0 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20240407_204325-gs0vw9iu/logs\u001b[0m\n",
      "/home/phil/work/mb/easybits/factory/.venv/lib/python3.11/site-packages/wandb/sdk/wandb_run.py:2265: UserWarning: Run (gs0vw9iu) is finished. The call to `_console_raw_callback` will be ignored. Please make sure that you are using an active run.\n",
      "  lambda data: self._console_raw_callback(\"stderr\", data),\n",
      "Steps: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 500/500 [04:28<00:00,  1.86it/s, loss=0.378, lr=0]\n"
     ]
    }
   ],
   "source": [
    "!accelerate launch --mixed-precision \"fp16\" train_dreambooth_lora.py \\\n",
    "  --pretrained_model_name_or_path='segmind/SSD-1B'  \\\n",
    "  --instance_data_dir='./datasets/jon_juarez/' \\\n",
    "  --output_dir='./models/ssd-jon_juarez-full-lora' \\\n",
    "  --instance_prompt=\"JON_JUAREZ\" \\\n",
    "  --resolution=512 \\\n",
    "  --train_batch_size=1 \\\n",
    "  --gradient_accumulation_steps=1 \\\n",
    "  --checkpointing_steps=100 \\\n",
    "  --learning_rate=1e-4 \\\n",
    "  --variant=\"fp16\" \\\n",
    "  --mixed_precision=\"fp16\" \\\n",
    "  --report_to=\"wandb\" \\\n",
    "  --lr_scheduler=\"cosine_with_restarts\" \\\n",
    "  --lr_warmup_steps=0 \\\n",
    "  --max_train_steps=500 \\\n",
    "  --seed=\"0\" \\\n",
    "  --push_to_hub \\\n",
    "  --hub_model_id \"philipp-zettl/ssd-jon_juarez-lora\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4IgQRvZniF3o"
   },
   "outputs": [],
   "source": [
    "!rm -rf train_lora.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from train_dreambooth_lora import *\n",
    "\n",
    "pretrained_model_name_or_path = 'segmind/tiny-sd'\n",
    "revision = None\n",
    "variant = None\n",
    "weight_dtype = torch.float16\n",
    "output_dir = './models/margot_robbie-full-lora/'\n",
    "logging_dir = 'logs/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_validation(\n",
    "    pipeline,\n",
    "    args,\n",
    "    accelerator,\n",
    "    pipeline_args,\n",
    "    epoch,\n",
    "    is_final_validation=False,\n",
    "):\n",
    "    logger.info(\n",
    "        f\"Running validation... \\n Generating {args.num_validation_images} images with prompt:\"\n",
    "        f\" {args.validation_prompt}.\"\n",
    "    )\n",
    "    # We train on the simplified learning objective. If we were previously predicting a variance, we need the scheduler to ignore it\n",
    "    scheduler_args = {}\n",
    "\n",
    "    if \"variance_type\" in pipeline.scheduler.config:\n",
    "        variance_type = pipeline.scheduler.config.variance_type\n",
    "\n",
    "        if variance_type in [\"learned\", \"learned_range\"]:\n",
    "            variance_type = \"fixed_small\"\n",
    "\n",
    "        scheduler_args[\"variance_type\"] = variance_type\n",
    "\n",
    "    pipeline.scheduler = DPMSolverMultistepScheduler.from_config(pipeline.scheduler.config, **scheduler_args)\n",
    "\n",
    "    pipeline = pipeline.to(accelerator.device)\n",
    "    pipeline.set_progress_bar_config(disable=True)\n",
    "\n",
    "    # run inference\n",
    "    generator = torch.Generator(device=accelerator.device).manual_seed(args.seed) if args.seed else None\n",
    "\n",
    "    images = []\n",
    "    for _ in range(args.num_validation_images):\n",
    "        with torch.cuda.amp.autocast():\n",
    "            image = pipeline(**pipeline_args, generator=generator).images[0]\n",
    "            images.append(image)\n",
    "\n",
    "    for tracker in accelerator.trackers:\n",
    "        phase_name = \"test\" if is_final_validation else \"validation\"\n",
    "        if tracker.name == \"tensorboard\":\n",
    "            np_images = np.stack([np.asarray(img) for img in images])\n",
    "            tracker.writer.add_images(phase_name, np_images, epoch, dataformats=\"NHWC\")\n",
    "        if tracker.name == \"wandb\":\n",
    "            tracker.log(\n",
    "                {\n",
    "                    phase_name: [\n",
    "                        wandb.Image(image, caption=f\"{i}: {args.validation_prompt}\") for i, image in enumerate(images)\n",
    "                    ]\n",
    "                }\n",
    "            )\n",
    "\n",
    "    del pipeline\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    return images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FooArgs:\n",
    "    num_validation_images = 10\n",
    "    seed = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "vae/diffusion_pytorch_model.safetensors not found\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dbb776d9b1284891860e198b56fe7961",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading pipeline components...:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pipeline = DiffusionPipeline.from_pretrained(\n",
    "    pretrained_model_name_or_path, revision=revision, variant=variant, torch_dtype=weight_dtype\n",
    ")\n",
    "\n",
    "# load attention processors\n",
    "pipeline.load_lora_weights(output_dir, weight_name=\"pytorch_lora_weights.safetensors\")\n",
    "\n",
    "logging_dir = Path(output_dir, logging_dir)\n",
    "accelerator_project_config = ProjectConfiguration(project_dir=output_dir, logging_dir=logging_dir)\n",
    "accelerator = Accelerator(\n",
    "    gradient_accumulation_steps=1,\n",
    "    mixed_precision='fp16',\n",
    "    log_with='wandb',\n",
    "    project_config=accelerator_project_config,\n",
    ")\n",
    "# run inference\n",
    "images = []\n",
    "validation_prompt = 'MAGROB style a woman in a field'\n",
    "\n",
    "if validation_prompt:\n",
    "    pipeline_args = {\"prompt\": validation_prompt, \"num_inference_steps\": 25}\n",
    "    images = log_validation(\n",
    "        pipeline,\n",
    "        FooArgs(),\n",
    "        accelerator,\n",
    "        pipeline_args,\n",
    "        500,\n",
    "        is_final_validation=True,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "vae/diffusion_pytorch_model.safetensors not found\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "abb50dd6f3334db5ad7958c4d199895d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading pipeline components...:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7c3046678f934b1aa20a06af00899d02",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "scheduler.bin:   0%|          | 0.00/1.00k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e181313391654ddb9bd7b2899e1e6a1e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Upload 21 LFS files:   0%|          | 0/21 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "466b01706f604ad4bf78ed3918cf481b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "random_states_0.pkl:   0%|          | 0.00/14.3k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a6dac151b1884c1f96e0267d54ac6b5b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "optimizer.bin:   0%|          | 0.00/3.57M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8154b4138b924335a67fe29bfdb80c9a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "optimizer.bin:   0%|          | 0.00/3.57M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f73b025f59274ee0b5655d74b203dcd0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_lora_weights.safetensors:   0%|          | 0.00/1.75M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "49445b87621948fda90b075dfbb604ff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_lora_weights.safetensors:   0%|          | 0.00/1.75M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "10927ff05ea3415ebe20ea8d4b6af035",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "random_states_0.pkl:   0%|          | 0.00/14.3k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "242070e75da1440fae057d4b39c64429",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "scheduler.bin:   0%|          | 0.00/1.00k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6bb398a55ba246a6939bd2686e86d8e2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_lora_weights.safetensors:   0%|          | 0.00/1.75M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "999c3bd0ef5a4b3fbe1157b7247c38c5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "optimizer.bin:   0%|          | 0.00/3.57M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b864da0fc8c04f0c95ede8a1415f4714",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "random_states_0.pkl:   0%|          | 0.00/14.3k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "321d652276174ca09da1979a7d191a99",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "scheduler.bin:   0%|          | 0.00/1.00k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8c6965e895324a9e85def9d7211f1a89",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "optimizer.bin:   0%|          | 0.00/3.57M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "88e86e6771b14b7bb5d5842763b849c0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_lora_weights.safetensors:   0%|          | 0.00/1.75M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "616717f3d35043bc8dfa94dff6fa25cd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "random_states_0.pkl:   0%|          | 0.00/14.3k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3b48923eb3864ae8853ca2a92b6d451e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "scheduler.bin:   0%|          | 0.00/1.00k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7f48594de8f34fcb80d7f9cb642186b3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "optimizer.bin:   0%|          | 0.00/3.57M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0b6c35325e8b48c38f4fbb9be49bdec1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_lora_weights.safetensors:   0%|          | 0.00/1.75M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a6eb474c54374c6c8897ac3f5e3189f9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "random_states_0.pkl:   0%|          | 0.00/14.3k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "df117721dfd94380bc3a194b06d5259b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "scheduler.bin:   0%|          | 0.00/1.00k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "da36e5cb4ef14efaab02d1b74ad7115b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_lora_weights.safetensors:   0%|          | 0.00/1.75M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/philipp-zettl/margot_robbie-lora/commit/9763b6bc9d090b59e81b579aea7a414eab1ed3c1', commit_message='End of training', commit_description='', oid='9763b6bc9d090b59e81b579aea7a414eab1ed3c1', pr_url=None, pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline = DiffusionPipeline.from_pretrained(\n",
    "    'segmind/tiny-sd',\n",
    "    torch_dtype=torch.float16,\n",
    "    safety_checker=None,\n",
    "    revision=None\n",
    ")\n",
    "save_model_card(\n",
    "    \"philipp-zettl/margot_robbie-lora\",\n",
    "    images=images,\n",
    "    base_model='segmind/tiny-sd',\n",
    "    train_text_encoder=False,\n",
    "    prompt=\"MAGROB style\",\n",
    "    repo_folder='./models/margot_robbie-full-lora/',\n",
    "    pipeline=pipeline,\n",
    ")\n",
    "upload_folder(\n",
    "    repo_id='philipp-zettl/margot_robbie-lora',\n",
    "    folder_path='./models/margot_robbie-full-lora/',\n",
    "    commit_message=\"End of training\",\n",
    "    ignore_patterns=[\"step_*\", \"epoch_*\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## different approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "!scp diffusers/examples/text_to_image/train_text_to_image_lora_sdxl.py ./train_text_to_image_lora_sdxl.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The following values were not passed to `accelerate launch` and had defaults used instead:\n",
      "\t`--num_processes` was set to a value of `1`\n",
      "\t`--num_machines` was set to a value of `1`\n",
      "\t`--mixed_precision` was set to a value of `'no'`\n",
      "\t`--dynamo_backend` was set to a value of `'no'`\n",
      "To avoid this warning pass in values for each of the problematic parameters or run `accelerate config`.\n",
      "/home/factory/.cache/pypoetry/virtualenvs/factory-HxwD1Xz5-py3.11/lib/python3.11/site-packages/accelerate/accelerator.py:391: UserWarning: `log_with=tensorboard` was passed but no supported trackers are currently installed.\n",
      "  warnings.warn(f\"`log_with={log_with}` was passed but no supported trackers are currently installed.\")\n",
      "04/07/2024 16:39:25 - INFO - __main__ - Distributed environment: DistributedType.NO\n",
      "Num processes: 1\n",
      "Process index: 0\n",
      "Local process index: 0\n",
      "Device: cuda\n",
      "\n",
      "Mixed precision type: no\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/factory/.cache/pypoetry/virtualenvs/factory-HxwD1Xz5-py3.11/lib/python3.11/site-packages/huggingface_hub/utils/_errors.py\", line 304, in hf_raise_for_status\n",
      "    response.raise_for_status()\n",
      "  File \"/home/factory/.cache/pypoetry/virtualenvs/factory-HxwD1Xz5-py3.11/lib/python3.11/site-packages/requests/models.py\", line 1021, in raise_for_status\n",
      "    raise HTTPError(http_error_msg, response=self)\n",
      "requests.exceptions.HTTPError: 404 Client Error: Not Found for url: https://huggingface.co/segmind/tiny-sd/resolve/main/tokenizer_2/config.json\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/factory/.cache/pypoetry/virtualenvs/factory-HxwD1Xz5-py3.11/lib/python3.11/site-packages/transformers/utils/hub.py\", line 398, in cached_file\n",
      "    resolved_file = hf_hub_download(\n",
      "                    ^^^^^^^^^^^^^^^^\n",
      "  File \"/home/factory/.cache/pypoetry/virtualenvs/factory-HxwD1Xz5-py3.11/lib/python3.11/site-packages/huggingface_hub/utils/_validators.py\", line 119, in _inner_fn\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/factory/.cache/pypoetry/virtualenvs/factory-HxwD1Xz5-py3.11/lib/python3.11/site-packages/huggingface_hub/file_download.py\", line 1261, in hf_hub_download\n",
      "    metadata = get_hf_file_metadata(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/factory/.cache/pypoetry/virtualenvs/factory-HxwD1Xz5-py3.11/lib/python3.11/site-packages/huggingface_hub/utils/_validators.py\", line 119, in _inner_fn\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/factory/.cache/pypoetry/virtualenvs/factory-HxwD1Xz5-py3.11/lib/python3.11/site-packages/huggingface_hub/file_download.py\", line 1674, in get_hf_file_metadata\n",
      "    r = _request_wrapper(\n",
      "        ^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/factory/.cache/pypoetry/virtualenvs/factory-HxwD1Xz5-py3.11/lib/python3.11/site-packages/huggingface_hub/file_download.py\", line 369, in _request_wrapper\n",
      "    response = _request_wrapper(\n",
      "               ^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/factory/.cache/pypoetry/virtualenvs/factory-HxwD1Xz5-py3.11/lib/python3.11/site-packages/huggingface_hub/file_download.py\", line 393, in _request_wrapper\n",
      "    hf_raise_for_status(response)\n",
      "  File \"/home/factory/.cache/pypoetry/virtualenvs/factory-HxwD1Xz5-py3.11/lib/python3.11/site-packages/huggingface_hub/utils/_errors.py\", line 315, in hf_raise_for_status\n",
      "    raise EntryNotFoundError(message, response) from e\n",
      "huggingface_hub.utils._errors.EntryNotFoundError: 404 Client Error. (Request ID: Root=1-6612b01d-62e2acf376a0693e376b1466;a86555af-6187-432f-9f43-19d9e49afe77)\n",
      "\n",
      "Entry Not Found for url: https://huggingface.co/segmind/tiny-sd/resolve/main/tokenizer_2/config.json.\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/factory/work/mb/factory/train/train_text_to_image_lora_sdxl.py\", line 1328, in <module>\n",
      "    main(args)\n",
      "  File \"/home/factory/work/mb/factory/train/train_text_to_image_lora_sdxl.py\", line 563, in main\n",
      "    tokenizer_two = AutoTokenizer.from_pretrained(\n",
      "                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/factory/.cache/pypoetry/virtualenvs/factory-HxwD1Xz5-py3.11/lib/python3.11/site-packages/transformers/models/auto/tokenization_auto.py\", line 794, in from_pretrained\n",
      "    config = AutoConfig.from_pretrained(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/factory/.cache/pypoetry/virtualenvs/factory-HxwD1Xz5-py3.11/lib/python3.11/site-packages/transformers/models/auto/configuration_auto.py\", line 1138, in from_pretrained\n",
      "    config_dict, unused_kwargs = PretrainedConfig.get_config_dict(pretrained_model_name_or_path, **kwargs)\n",
      "                                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/factory/.cache/pypoetry/virtualenvs/factory-HxwD1Xz5-py3.11/lib/python3.11/site-packages/transformers/configuration_utils.py\", line 631, in get_config_dict\n",
      "    config_dict, kwargs = cls._get_config_dict(pretrained_model_name_or_path, **kwargs)\n",
      "                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/factory/.cache/pypoetry/virtualenvs/factory-HxwD1Xz5-py3.11/lib/python3.11/site-packages/transformers/configuration_utils.py\", line 686, in _get_config_dict\n",
      "    resolved_config_file = cached_file(\n",
      "                           ^^^^^^^^^^^^\n",
      "  File \"/home/factory/.cache/pypoetry/virtualenvs/factory-HxwD1Xz5-py3.11/lib/python3.11/site-packages/transformers/utils/hub.py\", line 452, in cached_file\n",
      "    raise EnvironmentError(\n",
      "OSError: segmind/tiny-sd does not appear to have a file named tokenizer_2/config.json. Checkout 'https://huggingface.co/segmind/tiny-sd/main' for available files.\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/factory/.cache/pypoetry/virtualenvs/factory-HxwD1Xz5-py3.11/bin/accelerate\", line 8, in <module>\n",
      "    sys.exit(main())\n",
      "             ^^^^^^\n",
      "  File \"/home/factory/.cache/pypoetry/virtualenvs/factory-HxwD1Xz5-py3.11/lib/python3.11/site-packages/accelerate/commands/accelerate_cli.py\", line 46, in main\n",
      "    args.func(args)\n",
      "  File \"/home/factory/.cache/pypoetry/virtualenvs/factory-HxwD1Xz5-py3.11/lib/python3.11/site-packages/accelerate/commands/launch.py\", line 1075, in launch_command\n",
      "    simple_launcher(args)\n",
      "  File \"/home/factory/.cache/pypoetry/virtualenvs/factory-HxwD1Xz5-py3.11/lib/python3.11/site-packages/accelerate/commands/launch.py\", line 681, in simple_launcher\n",
      "    raise subprocess.CalledProcessError(returncode=process.returncode, cmd=cmd)\n",
      "subprocess.CalledProcessError: Command '['/home/factory/.cache/pypoetry/virtualenvs/factory-HxwD1Xz5-py3.11/bin/python', 'train_text_to_image_lora_sdxl.py', '--pretrained_model_name_or_path=segmind/tiny-sd', '--pretrained_vae_model_name_or_path=segmind/tiny-sd', '--dataset_name=philipp-zettl/jon_juarez', '--caption_column=text', '--resolution=1024', '--random_flip', '--train_batch_size=1', '--num_train_epochs=2', '--checkpointing_steps=500', '--learning_rate=1e-04', '--lr_scheduler=constant', '--lr_warmup_steps=0']' returned non-zero exit status 1.\n"
     ]
    }
   ],
   "source": [
    "!accelerate launch train_text_to_image_lora_sdxl.py \\\n",
    "  --pretrained_model_name_or_path=\"segmind/tiny-sd\" \\\n",
    "  --pretrained_vae_model_name_or_path=\"segmind/tiny-sd\" \\\n",
    "  --dataset_name='philipp-zettl/jon_juarez' --caption_column=\"text\" \\\n",
    "  --resolution=1024 --random_flip \\\n",
    "  --train_batch_size=1 \\\n",
    "  --num_train_epochs=2 --checkpointing_steps=500 \\\n",
    "  --learning_rate=1e-04 --lr_scheduler=\"constant\" --lr_warmup_steps=0 \\\n",
    "  #--mixed_precision=\"fp16\" \\\n",
    "  --seed=42 \\\n",
    "  --output_dir=\"tiny-sd-jon_juarez-lora\" \\\n",
    "  --validation_prompt=\"cute dragon creature\" --report_to=\"wandb\" \\\n",
    "  --push_to_hub \\\n",
    "  --hub_model_id \"philipp-zettl/sd-jon_juarez-model-lora-ssd\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The following values were not passed to `accelerate launch` and had defaults used instead:\n",
      "\t`--num_processes` was set to a value of `1`\n",
      "\t`--num_machines` was set to a value of `1`\n",
      "\t`--mixed_precision` was set to a value of `'no'`\n",
      "\t`--dynamo_backend` was set to a value of `'no'`\n",
      "To avoid this warning pass in values for each of the problematic parameters or run `accelerate config`.\n",
      "04/07/2024 16:42:18 - INFO - __main__ - Distributed environment: DistributedType.NO\n",
      "Num processes: 1\n",
      "Process index: 0\n",
      "Local process index: 0\n",
      "Device: cuda\n",
      "\n",
      "Mixed precision type: no\n",
      "\n",
      "You are using a model of type clip_text_model to instantiate a model of type . This is not supported for all configurations of models and can yield errors.\n",
      "{'rescale_betas_zero_snr', 'variance_type', 'dynamic_thresholding_ratio', 'thresholding', 'clip_sample_range'} was not found in config. Values will be initialized to default values.\n",
      "vae/config.json: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 602/602 [00:00<00:00, 2.16MB/s]\n",
      "vae/diffusion_pytorch_model.safetensors: 100%|‚ñà| 335M/335M [00:30<00:00, 10.9MB/\n",
      "{'latents_std', 'latents_mean'} was not found in config. Values will be initialized to default values.\n",
      "{'dropout', 'attention_type'} was not found in config. Values will be initialized to default values.\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/factory/work/mb/factory/train/train_dreambooth_lora.py\", line 1429, in <module>\n",
      "    main(args)\n",
      "  File \"/home/factory/work/mb/factory/train/train_dreambooth_lora.py\", line 893, in main\n",
      "    vae.to(accelerator.device, dtype=weight_dtype)\n",
      "  File \"/home/factory/.cache/pypoetry/virtualenvs/factory-HxwD1Xz5-py3.11/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1152, in to\n",
      "    return self._apply(convert)\n",
      "           ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/factory/.cache/pypoetry/virtualenvs/factory-HxwD1Xz5-py3.11/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 802, in _apply\n",
      "    module._apply(fn)\n",
      "  File \"/home/factory/.cache/pypoetry/virtualenvs/factory-HxwD1Xz5-py3.11/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 802, in _apply\n",
      "    module._apply(fn)\n",
      "  File \"/home/factory/.cache/pypoetry/virtualenvs/factory-HxwD1Xz5-py3.11/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 802, in _apply\n",
      "    module._apply(fn)\n",
      "  [Previous line repeated 3 more times]\n",
      "  File \"/home/factory/.cache/pypoetry/virtualenvs/factory-HxwD1Xz5-py3.11/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 825, in _apply\n",
      "    param_applied = fn(param)\n",
      "                    ^^^^^^^^^\n",
      "  File \"/home/factory/.cache/pypoetry/virtualenvs/factory-HxwD1Xz5-py3.11/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1150, in convert\n",
      "    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 5.78 GiB of which 22.31 MiB is free. Including non-PyTorch memory, this process has 5.33 GiB memory in use. Of the allocated memory 5.14 GiB is allocated by PyTorch, and 117.23 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/factory/.cache/pypoetry/virtualenvs/factory-HxwD1Xz5-py3.11/bin/accelerate\", line 8, in <module>\n",
      "    sys.exit(main())\n",
      "             ^^^^^^\n",
      "  File \"/home/factory/.cache/pypoetry/virtualenvs/factory-HxwD1Xz5-py3.11/lib/python3.11/site-packages/accelerate/commands/accelerate_cli.py\", line 46, in main\n",
      "    args.func(args)\n",
      "  File \"/home/factory/.cache/pypoetry/virtualenvs/factory-HxwD1Xz5-py3.11/lib/python3.11/site-packages/accelerate/commands/launch.py\", line 1075, in launch_command\n",
      "    simple_launcher(args)\n",
      "  File \"/home/factory/.cache/pypoetry/virtualenvs/factory-HxwD1Xz5-py3.11/lib/python3.11/site-packages/accelerate/commands/launch.py\", line 681, in simple_launcher\n",
      "    raise subprocess.CalledProcessError(returncode=process.returncode, cmd=cmd)\n",
      "subprocess.CalledProcessError: Command '['/home/factory/.cache/pypoetry/virtualenvs/factory-HxwD1Xz5-py3.11/bin/python', 'train_dreambooth_lora.py', '--pretrained_model_name_or_path=segmind/SSD-1B', '--instance_data_dir=./datasets/margot_robbie', '--output_dir=./models/ssd-margot_robbie-full-lora', '--instance_prompt=MAGROB style', '--class_data_dir=./datasets/image_data', '--class_prompt=style', '--with_prior_preservation', '--num_class_images=10', '--resolution=512', '--train_batch_size=1', '--gradient_accumulation_steps=1', '--checkpointing_steps=100', '--learning_rate=5e-4', '--report_to=wandb', '--lr_scheduler=polynomial', '--lr_warmup_steps=0', '--max_train_steps=500', '--validation_prompt=MAGROB style woman on a field of flowers', '--validation_epochs=5', '--seed=0', '--push_to_hub', '--hub_model_id', 'philipp-zettl/ssd-margot_robbie-lora']' returned non-zero exit status 1.\n"
     ]
    }
   ],
   "source": [
    "!accelerate launch train_dreambooth_lora.py \\\n",
    "  --pretrained_model_name_or_path='segmind/SSD-1B'  \\\n",
    "  --instance_data_dir='./datasets/margot_robbie' \\\n",
    "  --output_dir='./models/ssd-margot_robbie-full-lora' \\\n",
    "  --instance_prompt=\"MAGROB style\" \\\n",
    "  --class_data_dir='./datasets/image_data' \\\n",
    "  --class_prompt='style' \\\n",
    "  --with_prior_preservation \\\n",
    "  --num_class_images=10 \\\n",
    "  --resolution=512 \\\n",
    "  --train_batch_size=1 \\\n",
    "  --gradient_accumulation_steps=1 \\\n",
    "  --checkpointing_steps=100 \\\n",
    "  --learning_rate=5e-4 \\\n",
    "  --report_to=\"wandb\" \\\n",
    "  --lr_scheduler=\"polynomial\" \\\n",
    "  --lr_warmup_steps=0 \\\n",
    "  --max_train_steps=500 \\\n",
    "  --validation_prompt=\"MAGROB style woman on a field of flowers\" \\\n",
    "  --validation_epochs=5 \\\n",
    "  --seed=\"0\" \\\n",
    "  --push_to_hub \\\n",
    "  --hub_model_id \"philipp-zettl/ssd-margot_robbie-lora\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next attempt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "!scp diffusers/examples/text_to_image/train_text_to_image_lora.py ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PosixPath('/home/phil/.cache/huggingface/accelerate/default_config.yaml')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from accelerate.utils import write_basic_config\n",
    "\n",
    "write_basic_config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "04/07/2024 17:31:55 - INFO - __main__ - Distributed environment: DistributedType.NO\n",
      "Num processes: 1\n",
      "Process index: 0\n",
      "Local process index: 0\n",
      "Device: cuda\n",
      "\n",
      "Mixed precision type: no\n",
      "\n",
      "{'rescale_betas_zero_snr'} was not found in config. Values will be initialized to default values.\n",
      "{'latents_mean', 'latents_std'} was not found in config. Values will be initialized to default values.\n",
      "{'resnet_out_scale_factor', 'cross_attention_norm', 'addition_embed_type', 'num_class_embeds', 'attention_type', 'addition_embed_type_num_heads', 'use_linear_projection', 'projection_class_embeddings_input_dim', 'encoder_hid_dim_type', 'addition_time_embed_dim', 'resnet_time_scale_shift', 'time_embedding_dim', 'conv_out_kernel', 'mid_block_scale_factor', 'dual_cross_attention', 'time_cond_proj_dim', 'upcast_attention', 'only_cross_attention', 'transformer_layers_per_block', 'class_embed_type', 'time_embedding_type', 'conv_in_kernel', 'mid_block_only_cross_attention', 'num_attention_heads', 'reverse_transformer_layers_per_block', 'time_embedding_act_fn', 'dropout', 'timestep_post_act', 'resnet_skip_time_act', 'class_embeddings_concat', 'encoder_hid_dim'} was not found in config. Values will be initialized to default values.\n",
      "Downloading readme: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 316/316 [00:00<00:00, 1.55MB/s]\n",
      "Downloading data: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 455M/455M [00:42<00:00, 10.8MB/s]\n",
      "Generating train split: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 438/438 [00:00<00:00, 468.18 examples/s]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mphilipp-zettl\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.16.6\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/home/factory/work/mb/factory/train/wandb/run-20240407_173248-v51n7dta\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mgrateful-lake-6\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ‚≠êÔ∏è View project at \u001b[34m\u001b[4mhttps://wandb.ai/philipp-zettl/text2image-fine-tune\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: üöÄ View run at \u001b[34m\u001b[4mhttps://wandb.ai/philipp-zettl/text2image-fine-tune/runs/v51n7dta\u001b[0m\n",
      "04/07/2024 17:32:49 - INFO - __main__ - ***** Running training *****\n",
      "04/07/2024 17:32:49 - INFO - __main__ -   Num examples = 438\n",
      "04/07/2024 17:32:49 - INFO - __main__ -   Num Epochs = 137\n",
      "04/07/2024 17:32:49 - INFO - __main__ -   Instantaneous batch size per device = 1\n",
      "04/07/2024 17:32:49 - INFO - __main__ -   Total train batch size (w. parallel, distributed & accumulation) = 4\n",
      "04/07/2024 17:32:49 - INFO - __main__ -   Gradient Accumulation steps = 4\n",
      "04/07/2024 17:32:49 - INFO - __main__ -   Total optimization steps = 15000\n",
      "Steps:   1%| | 110/15000 [04:18<8:31:34,  2.06s/it, lr=0.0001, step_loss=0.0030104/07/2024 17:37:08 - INFO - __main__ - Running validation... \n",
      " Generating 4 images with prompt: SUPER_PROMPT a nude woman on the beach.\n",
      "vae/diffusion_pytorch_model.safetensors not found\n",
      "{'image_encoder'} was not found in config. Values will be initialized to default values.\n",
      "\n",
      "Loading pipeline components...:   0%|                     | 0/5 [00:00<?, ?it/s]\u001b[A{'latents_mean', 'latents_std'} was not found in config. Values will be initialized to default values.\n",
      "Loaded vae as AutoencoderKL from `vae` subfolder of segmind/tiny-sd.\n",
      "\n",
      "Loading pipeline components...:  20%|‚ñà‚ñà‚ñå          | 1/5 [00:00<00:00,  4.21it/s]\u001b[A{'euler_at_final', 'final_sigmas_type', 'rescale_betas_zero_snr', 'use_lu_lambdas'} was not found in config. Values will be initialized to default values.\n",
      "Loaded scheduler as DPMSolverMultistepScheduler from `scheduler` subfolder of segmind/tiny-sd.\n",
      "Loaded tokenizer as CLIPTokenizer from `tokenizer` subfolder of segmind/tiny-sd.\n",
      "Loaded text_encoder as CLIPTextModel from `text_encoder` subfolder of segmind/tiny-sd.\n",
      "\n",
      "Loading pipeline components...: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5/5 [00:00<00:00,  8.06it/s]\u001b[A\n",
      "Steps:   1%| | 220/15000 [08:54<8:13:29,  2.00s/it, lr=9.99e-5, step_loss=0.024304/07/2024 17:41:43 - INFO - __main__ - Running validation... \n",
      " Generating 4 images with prompt: SUPER_PROMPT a nude woman on the beach.\n",
      "vae/diffusion_pytorch_model.safetensors not found\n",
      "{'image_encoder'} was not found in config. Values will be initialized to default values.\n",
      "\n",
      "Loading pipeline components...:   0%|                     | 0/5 [00:00<?, ?it/s]\u001b[A{'latents_mean', 'latents_std'} was not found in config. Values will be initialized to default values.\n",
      "Loaded vae as AutoencoderKL from `vae` subfolder of segmind/tiny-sd.\n",
      "\n",
      "Loading pipeline components...:  20%|‚ñà‚ñà‚ñå          | 1/5 [00:00<00:01,  3.64it/s]\u001b[A{'euler_at_final', 'final_sigmas_type', 'rescale_betas_zero_snr', 'use_lu_lambdas'} was not found in config. Values will be initialized to default values.\n",
      "Loaded scheduler as DPMSolverMultistepScheduler from `scheduler` subfolder of segmind/tiny-sd.\n",
      "Loaded tokenizer as CLIPTokenizer from `tokenizer` subfolder of segmind/tiny-sd.\n",
      "Loaded text_encoder as CLIPTextModel from `text_encoder` subfolder of segmind/tiny-sd.\n",
      "\n",
      "Loading pipeline components...: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5/5 [00:00<00:00,  7.52it/s]\u001b[A\n",
      "Steps:   2%| | 330/15000 [13:25<8:12:59,  2.02s/it, lr=9.99e-5, step_loss=0.003804/07/2024 17:46:15 - INFO - __main__ - Running validation... \n",
      " Generating 4 images with prompt: SUPER_PROMPT a nude woman on the beach.\n",
      "vae/diffusion_pytorch_model.safetensors not found\n",
      "{'image_encoder'} was not found in config. Values will be initialized to default values.\n",
      "\n",
      "Loading pipeline components...:   0%|                     | 0/5 [00:00<?, ?it/s]\u001b[A{'latents_mean', 'latents_std'} was not found in config. Values will be initialized to default values.\n",
      "Loaded vae as AutoencoderKL from `vae` subfolder of segmind/tiny-sd.\n",
      "\n",
      "Loading pipeline components...:  20%|‚ñà‚ñà‚ñå          | 1/5 [00:00<00:01,  3.46it/s]\u001b[A{'euler_at_final', 'final_sigmas_type', 'rescale_betas_zero_snr', 'use_lu_lambdas'} was not found in config. Values will be initialized to default values.\n",
      "Loaded scheduler as DPMSolverMultistepScheduler from `scheduler` subfolder of segmind/tiny-sd.\n",
      "Loaded tokenizer as CLIPTokenizer from `tokenizer` subfolder of segmind/tiny-sd.\n",
      "Loaded text_encoder as CLIPTextModel from `text_encoder` subfolder of segmind/tiny-sd.\n",
      "\n",
      "Loading pipeline components...: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5/5 [00:00<00:00,  7.44it/s]\u001b[A\n",
      "Steps:   3%| | 440/15000 [18:15<9:44:35,  2.41s/it, lr=9.98e-5, step_loss=0.193]04/07/2024 17:51:05 - INFO - __main__ - Running validation... \n",
      " Generating 4 images with prompt: SUPER_PROMPT a nude woman on the beach.\n",
      "vae/diffusion_pytorch_model.safetensors not found\n",
      "{'image_encoder'} was not found in config. Values will be initialized to default values.\n",
      "\n",
      "Loading pipeline components...:   0%|                     | 0/5 [00:00<?, ?it/s]\u001b[A{'latents_mean', 'latents_std'} was not found in config. Values will be initialized to default values.\n",
      "Loaded vae as AutoencoderKL from `vae` subfolder of segmind/tiny-sd.\n",
      "\n",
      "Loading pipeline components...:  20%|‚ñà‚ñà‚ñå          | 1/5 [00:00<00:00,  4.31it/s]\u001b[A{'euler_at_final', 'final_sigmas_type', 'rescale_betas_zero_snr', 'use_lu_lambdas'} was not found in config. Values will be initialized to default values.\n",
      "Loaded scheduler as DPMSolverMultistepScheduler from `scheduler` subfolder of segmind/tiny-sd.\n",
      "Loaded tokenizer as CLIPTokenizer from `tokenizer` subfolder of segmind/tiny-sd.\n",
      "Loaded text_encoder as CLIPTextModel from `text_encoder` subfolder of segmind/tiny-sd.\n",
      "\n",
      "Loading pipeline components...: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5/5 [00:00<00:00,  7.97it/s]\u001b[A\n",
      "Steps:   3%| | 500/15000 [21:21<11:14:03,  2.79s/it, lr=9.97e-5, step_loss=0.00904/07/2024 17:54:10 - INFO - accelerate.accelerator - Saving current state to ./finetune/lora/nsfw/checkpoint-500\n",
      "04/07/2024 17:54:12 - INFO - accelerate.checkpointing - Model weights saved in finetune/lora/nsfw/checkpoint-500/model.safetensors\n",
      "04/07/2024 17:54:12 - INFO - accelerate.checkpointing - Optimizer state saved in finetune/lora/nsfw/checkpoint-500/optimizer.bin\n",
      "04/07/2024 17:54:12 - INFO - accelerate.checkpointing - Scheduler state saved in finetune/lora/nsfw/checkpoint-500/scheduler.bin\n",
      "04/07/2024 17:54:12 - INFO - accelerate.checkpointing - Sampler state for dataloader 0 saved in finetune/lora/nsfw/checkpoint-500/sampler.bin\n",
      "04/07/2024 17:54:12 - INFO - accelerate.checkpointing - Random states saved in finetune/lora/nsfw/checkpoint-500/random_states_0.pkl\n",
      "Model weights saved in finetune/lora/nsfw/checkpoint-500/pytorch_lora_weights.safetensors\n",
      "04/07/2024 17:54:12 - INFO - __main__ - Saved state to ./finetune/lora/nsfw/checkpoint-500\n",
      "Steps:   4%| | 550/15000 [23:41<9:39:22,  2.41s/it, lr=9.97e-5, step_loss=0.022204/07/2024 17:56:30 - INFO - __main__ - Running validation... \n",
      " Generating 4 images with prompt: SUPER_PROMPT a nude woman on the beach.\n",
      "vae/diffusion_pytorch_model.safetensors not found\n",
      "{'image_encoder'} was not found in config. Values will be initialized to default values.\n",
      "\n",
      "Loading pipeline components...:   0%|                     | 0/5 [00:00<?, ?it/s]\u001b[A{'latents_mean', 'latents_std'} was not found in config. Values will be initialized to default values.\n",
      "Loaded vae as AutoencoderKL from `vae` subfolder of segmind/tiny-sd.\n",
      "\n",
      "Loading pipeline components...:  20%|‚ñà‚ñà‚ñå          | 1/5 [00:00<00:00,  5.42it/s]\u001b[A{'euler_at_final', 'final_sigmas_type', 'rescale_betas_zero_snr', 'use_lu_lambdas'} was not found in config. Values will be initialized to default values.\n",
      "Loaded scheduler as DPMSolverMultistepScheduler from `scheduler` subfolder of segmind/tiny-sd.\n",
      "Loaded tokenizer as CLIPTokenizer from `tokenizer` subfolder of segmind/tiny-sd.\n",
      "Loaded text_encoder as CLIPTextModel from `text_encoder` subfolder of segmind/tiny-sd.\n",
      "\n",
      "Loading pipeline components...: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5/5 [00:00<00:00,  8.71it/s]\u001b[A\n",
      "Steps:   4%| | 660/15000 [29:04<9:31:30,  2.39s/it, lr=9.95e-5, step_loss=0.003904/07/2024 18:01:54 - INFO - __main__ - Running validation... \n",
      " Generating 4 images with prompt: SUPER_PROMPT a nude woman on the beach.\n",
      "vae/diffusion_pytorch_model.safetensors not found\n",
      "{'image_encoder'} was not found in config. Values will be initialized to default values.\n",
      "\n",
      "Loading pipeline components...:   0%|                     | 0/5 [00:00<?, ?it/s]\u001b[A{'latents_mean', 'latents_std'} was not found in config. Values will be initialized to default values.\n",
      "Loaded vae as AutoencoderKL from `vae` subfolder of segmind/tiny-sd.\n",
      "\n",
      "Loading pipeline components...:  20%|‚ñà‚ñà‚ñå          | 1/5 [00:00<00:00,  4.10it/s]\u001b[A{'euler_at_final', 'final_sigmas_type', 'rescale_betas_zero_snr', 'use_lu_lambdas'} was not found in config. Values will be initialized to default values.\n",
      "Loaded scheduler as DPMSolverMultistepScheduler from `scheduler` subfolder of segmind/tiny-sd.\n",
      "Loaded tokenizer as CLIPTokenizer from `tokenizer` subfolder of segmind/tiny-sd.\n",
      "Loaded text_encoder as CLIPTextModel from `text_encoder` subfolder of segmind/tiny-sd.\n",
      "\n",
      "Loading pipeline components...: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5/5 [00:00<00:00,  8.12it/s]\u001b[A\n",
      "Steps:   5%|  | 770/15000 [34:28<9:34:52,  2.42s/it, lr=9.94e-5, step_loss=0.26]04/07/2024 18:07:18 - INFO - __main__ - Running validation... \n",
      " Generating 4 images with prompt: SUPER_PROMPT a nude woman on the beach.\n",
      "vae/diffusion_pytorch_model.safetensors not found\n",
      "{'image_encoder'} was not found in config. Values will be initialized to default values.\n",
      "\n",
      "Loading pipeline components...:   0%|                     | 0/5 [00:00<?, ?it/s]\u001b[A{'latents_mean', 'latents_std'} was not found in config. Values will be initialized to default values.\n",
      "Loaded vae as AutoencoderKL from `vae` subfolder of segmind/tiny-sd.\n",
      "\n",
      "Loading pipeline components...:  20%|‚ñà‚ñà‚ñå          | 1/5 [00:00<00:01,  3.57it/s]\u001b[A{'euler_at_final', 'final_sigmas_type', 'rescale_betas_zero_snr', 'use_lu_lambdas'} was not found in config. Values will be initialized to default values.\n",
      "Loaded scheduler as DPMSolverMultistepScheduler from `scheduler` subfolder of segmind/tiny-sd.\n",
      "Loaded tokenizer as CLIPTokenizer from `tokenizer` subfolder of segmind/tiny-sd.\n",
      "Loaded text_encoder as CLIPTextModel from `text_encoder` subfolder of segmind/tiny-sd.\n",
      "\n",
      "Loading pipeline components...: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5/5 [00:00<00:00,  7.51it/s]\u001b[A\n",
      "Steps:   6%| | 880/15000 [39:52<9:25:27,  2.40s/it, lr=9.92e-5, step_loss=0.094304/07/2024 18:12:41 - INFO - __main__ - Running validation... \n",
      " Generating 4 images with prompt: SUPER_PROMPT a nude woman on the beach.\n",
      "vae/diffusion_pytorch_model.safetensors not found\n",
      "{'image_encoder'} was not found in config. Values will be initialized to default values.\n",
      "\n",
      "Loading pipeline components...:   0%|                     | 0/5 [00:00<?, ?it/s]\u001b[A{'latents_mean', 'latents_std'} was not found in config. Values will be initialized to default values.\n",
      "Loaded vae as AutoencoderKL from `vae` subfolder of segmind/tiny-sd.\n",
      "\n",
      "Loading pipeline components...:  20%|‚ñà‚ñà‚ñå          | 1/5 [00:00<00:01,  3.53it/s]\u001b[A{'euler_at_final', 'final_sigmas_type', 'rescale_betas_zero_snr', 'use_lu_lambdas'} was not found in config. Values will be initialized to default values.\n",
      "Loaded scheduler as DPMSolverMultistepScheduler from `scheduler` subfolder of segmind/tiny-sd.\n",
      "Loaded tokenizer as CLIPTokenizer from `tokenizer` subfolder of segmind/tiny-sd.\n",
      "Loaded text_encoder as CLIPTextModel from `text_encoder` subfolder of segmind/tiny-sd.\n",
      "\n",
      "Loading pipeline components...: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5/5 [00:00<00:00,  7.49it/s]\u001b[A\n",
      "Steps:   7%| | 990/15000 [44:43<7:49:20,  2.01s/it, lr=9.89e-5, step_loss=0.015104/07/2024 18:17:33 - INFO - __main__ - Running validation... \n",
      " Generating 4 images with prompt: SUPER_PROMPT a nude woman on the beach.\n",
      "vae/diffusion_pytorch_model.safetensors not found\n",
      "{'image_encoder'} was not found in config. Values will be initialized to default values.\n",
      "\n",
      "Loading pipeline components...:   0%|                     | 0/5 [00:00<?, ?it/s]\u001b[A{'latents_mean', 'latents_std'} was not found in config. Values will be initialized to default values.\n",
      "Loaded vae as AutoencoderKL from `vae` subfolder of segmind/tiny-sd.\n",
      "\n",
      "Loading pipeline components...:  20%|‚ñà‚ñà‚ñå          | 1/5 [00:00<00:01,  3.60it/s]\u001b[A{'euler_at_final', 'final_sigmas_type', 'rescale_betas_zero_snr', 'use_lu_lambdas'} was not found in config. Values will be initialized to default values.\n",
      "Loaded scheduler as DPMSolverMultistepScheduler from `scheduler` subfolder of segmind/tiny-sd.\n",
      "Loaded tokenizer as CLIPTokenizer from `tokenizer` subfolder of segmind/tiny-sd.\n",
      "Loaded text_encoder as CLIPTextModel from `text_encoder` subfolder of segmind/tiny-sd.\n",
      "\n",
      "Loading pipeline components...: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5/5 [00:00<00:00,  7.58it/s]\u001b[A\n",
      "Steps:   7%| | 1000/15000 [45:21<9:46:27,  2.51s/it, lr=9.89e-5, step_loss=0.04104/07/2024 18:18:11 - INFO - accelerate.accelerator - Saving current state to ./finetune/lora/nsfw/checkpoint-1000\n",
      "04/07/2024 18:18:13 - INFO - accelerate.checkpointing - Model weights saved in finetune/lora/nsfw/checkpoint-1000/model.safetensors\n",
      "04/07/2024 18:18:13 - INFO - accelerate.checkpointing - Optimizer state saved in finetune/lora/nsfw/checkpoint-1000/optimizer.bin\n",
      "04/07/2024 18:18:13 - INFO - accelerate.checkpointing - Scheduler state saved in finetune/lora/nsfw/checkpoint-1000/scheduler.bin\n",
      "04/07/2024 18:18:13 - INFO - accelerate.checkpointing - Sampler state for dataloader 0 saved in finetune/lora/nsfw/checkpoint-1000/sampler.bin\n",
      "04/07/2024 18:18:13 - INFO - accelerate.checkpointing - Random states saved in finetune/lora/nsfw/checkpoint-1000/random_states_0.pkl\n",
      "Model weights saved in finetune/lora/nsfw/checkpoint-1000/pytorch_lora_weights.safetensors\n",
      "04/07/2024 18:18:13 - INFO - __main__ - Saved state to ./finetune/lora/nsfw/checkpoint-1000\n",
      "Steps:   7%| | 1100/15000 [49:16<7:45:14,  2.01s/it, lr=9.87e-5, step_loss=0.04604/07/2024 18:22:06 - INFO - __main__ - Running validation... \n",
      " Generating 4 images with prompt: SUPER_PROMPT a nude woman on the beach.\n",
      "vae/diffusion_pytorch_model.safetensors not found\n",
      "{'image_encoder'} was not found in config. Values will be initialized to default values.\n",
      "\n",
      "Loading pipeline components...:   0%|                     | 0/5 [00:00<?, ?it/s]\u001b[A{'latents_mean', 'latents_std'} was not found in config. Values will be initialized to default values.\n",
      "Loaded vae as AutoencoderKL from `vae` subfolder of segmind/tiny-sd.\n",
      "\n",
      "Loading pipeline components...:  20%|‚ñà‚ñà‚ñå          | 1/5 [00:00<00:01,  2.82it/s]\u001b[A{'euler_at_final', 'final_sigmas_type', 'rescale_betas_zero_snr', 'use_lu_lambdas'} was not found in config. Values will be initialized to default values.\n",
      "Loaded scheduler as DPMSolverMultistepScheduler from `scheduler` subfolder of segmind/tiny-sd.\n",
      "Loaded tokenizer as CLIPTokenizer from `tokenizer` subfolder of segmind/tiny-sd.\n",
      "Loaded text_encoder as CLIPTextModel from `text_encoder` subfolder of segmind/tiny-sd.\n",
      "\n",
      "Loading pipeline components...: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5/5 [00:00<00:00,  6.70it/s]\u001b[A\n",
      "Steps:   8%| | 1210/15000 [53:47<7:46:03,  2.03s/it, lr=9.84e-5, step_loss=0.20604/07/2024 18:26:37 - INFO - __main__ - Running validation... \n",
      " Generating 4 images with prompt: SUPER_PROMPT a nude woman on the beach.\n",
      "vae/diffusion_pytorch_model.safetensors not found\n",
      "{'image_encoder'} was not found in config. Values will be initialized to default values.\n",
      "\n",
      "Loading pipeline components...:   0%|                     | 0/5 [00:00<?, ?it/s]\u001b[A{'latents_mean', 'latents_std'} was not found in config. Values will be initialized to default values.\n",
      "Loaded vae as AutoencoderKL from `vae` subfolder of segmind/tiny-sd.\n",
      "\n",
      "Loading pipeline components...:  20%|‚ñà‚ñà‚ñå          | 1/5 [00:00<00:01,  2.83it/s]\u001b[A{'euler_at_final', 'final_sigmas_type', 'rescale_betas_zero_snr', 'use_lu_lambdas'} was not found in config. Values will be initialized to default values.\n",
      "Loaded scheduler as DPMSolverMultistepScheduler from `scheduler` subfolder of segmind/tiny-sd.\n",
      "Loaded tokenizer as CLIPTokenizer from `tokenizer` subfolder of segmind/tiny-sd.\n",
      "Loaded text_encoder as CLIPTextModel from `text_encoder` subfolder of segmind/tiny-sd.\n",
      "\n",
      "Loading pipeline components...: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5/5 [00:00<00:00,  6.79it/s]\u001b[A\n",
      "Steps:   9%| | 1320/15000 [58:49<9:08:22,  2.41s/it, lr=9.81e-5, step_loss=0.35804/07/2024 18:31:39 - INFO - __main__ - Running validation... \n",
      " Generating 4 images with prompt: SUPER_PROMPT a nude woman on the beach.\n",
      "vae/diffusion_pytorch_model.safetensors not found\n",
      "{'image_encoder'} was not found in config. Values will be initialized to default values.\n",
      "\n",
      "Loading pipeline components...:   0%|                     | 0/5 [00:00<?, ?it/s]\u001b[A{'latents_mean', 'latents_std'} was not found in config. Values will be initialized to default values.\n",
      "Loaded vae as AutoencoderKL from `vae` subfolder of segmind/tiny-sd.\n",
      "\n",
      "Loading pipeline components...:  20%|‚ñà‚ñà‚ñå          | 1/5 [00:00<00:01,  2.76it/s]\u001b[A{'euler_at_final', 'final_sigmas_type', 'rescale_betas_zero_snr', 'use_lu_lambdas'} was not found in config. Values will be initialized to default values.\n",
      "Loaded scheduler as DPMSolverMultistepScheduler from `scheduler` subfolder of segmind/tiny-sd.\n",
      "Loaded tokenizer as CLIPTokenizer from `tokenizer` subfolder of segmind/tiny-sd.\n",
      "Loaded text_encoder as CLIPTextModel from `text_encoder` subfolder of segmind/tiny-sd.\n",
      "\n",
      "Loading pipeline components...: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5/5 [00:00<00:00,  6.62it/s]\u001b[A\n",
      "Steps:  10%| | 1430/15000 [1:04:13<9:05:21,  2.41s/it, lr=9.78e-5, step_loss=0.004/07/2024 18:37:02 - INFO - __main__ - Running validation... \n",
      " Generating 4 images with prompt: SUPER_PROMPT a nude woman on the beach.\n",
      "vae/diffusion_pytorch_model.safetensors not found\n",
      "{'image_encoder'} was not found in config. Values will be initialized to default values.\n",
      "\n",
      "Loading pipeline components...:   0%|                     | 0/5 [00:00<?, ?it/s]\u001b[A{'latents_mean', 'latents_std'} was not found in config. Values will be initialized to default values.\n",
      "Loaded vae as AutoencoderKL from `vae` subfolder of segmind/tiny-sd.\n",
      "\n",
      "Loading pipeline components...:  20%|‚ñà‚ñà‚ñå          | 1/5 [00:00<00:01,  2.83it/s]\u001b[A{'euler_at_final', 'final_sigmas_type', 'rescale_betas_zero_snr', 'use_lu_lambdas'} was not found in config. Values will be initialized to default values.\n",
      "Loaded scheduler as DPMSolverMultistepScheduler from `scheduler` subfolder of segmind/tiny-sd.\n",
      "Loaded tokenizer as CLIPTokenizer from `tokenizer` subfolder of segmind/tiny-sd.\n",
      "Loaded text_encoder as CLIPTextModel from `text_encoder` subfolder of segmind/tiny-sd.\n",
      "\n",
      "Loading pipeline components...: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5/5 [00:00<00:00,  6.81it/s]\u001b[A\n",
      "Steps:  10%| | 1500/15000 [1:07:45<10:25:20,  2.78s/it, lr=9.76e-5, step_loss=0.04/07/2024 18:40:35 - INFO - accelerate.accelerator - Saving current state to ./finetune/lora/nsfw/checkpoint-1500\n",
      "04/07/2024 18:40:37 - INFO - accelerate.checkpointing - Model weights saved in finetune/lora/nsfw/checkpoint-1500/model.safetensors\n",
      "04/07/2024 18:40:37 - INFO - accelerate.checkpointing - Optimizer state saved in finetune/lora/nsfw/checkpoint-1500/optimizer.bin\n",
      "04/07/2024 18:40:37 - INFO - accelerate.checkpointing - Scheduler state saved in finetune/lora/nsfw/checkpoint-1500/scheduler.bin\n",
      "04/07/2024 18:40:37 - INFO - accelerate.checkpointing - Sampler state for dataloader 0 saved in finetune/lora/nsfw/checkpoint-1500/sampler.bin\n",
      "04/07/2024 18:40:37 - INFO - accelerate.checkpointing - Random states saved in finetune/lora/nsfw/checkpoint-1500/random_states_0.pkl\n",
      "Model weights saved in finetune/lora/nsfw/checkpoint-1500/pytorch_lora_weights.safetensors\n",
      "04/07/2024 18:40:37 - INFO - __main__ - Saved state to ./finetune/lora/nsfw/checkpoint-1500\n",
      "Steps:  10%| | 1540/15000 [1:09:37<8:59:34,  2.41s/it, lr=9.74e-5, step_loss=0.204/07/2024 18:42:27 - INFO - __main__ - Running validation... \n",
      " Generating 4 images with prompt: SUPER_PROMPT a nude woman on the beach.\n",
      "vae/diffusion_pytorch_model.safetensors not found\n",
      "{'image_encoder'} was not found in config. Values will be initialized to default values.\n",
      "\n",
      "Loading pipeline components...:   0%|                     | 0/5 [00:00<?, ?it/s]\u001b[A{'latents_mean', 'latents_std'} was not found in config. Values will be initialized to default values.\n",
      "Loaded vae as AutoencoderKL from `vae` subfolder of segmind/tiny-sd.\n",
      "\n",
      "Loading pipeline components...:  20%|‚ñà‚ñà‚ñå          | 1/5 [00:00<00:01,  2.89it/s]\u001b[A{'euler_at_final', 'final_sigmas_type', 'rescale_betas_zero_snr', 'use_lu_lambdas'} was not found in config. Values will be initialized to default values.\n",
      "Loaded scheduler as DPMSolverMultistepScheduler from `scheduler` subfolder of segmind/tiny-sd.\n",
      "Loaded tokenizer as CLIPTokenizer from `tokenizer` subfolder of segmind/tiny-sd.\n",
      "Loaded text_encoder as CLIPTextModel from `text_encoder` subfolder of segmind/tiny-sd.\n",
      "\n",
      "Loading pipeline components...: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5/5 [00:00<00:00,  6.86it/s]\u001b[A\n",
      "Steps:  11%| | 1650/15000 [1:15:01<8:55:30,  2.41s/it, lr=9.7e-5, step_loss=0.0204/07/2024 18:47:51 - INFO - __main__ - Running validation... \n",
      " Generating 4 images with prompt: SUPER_PROMPT a nude woman on the beach.\n",
      "vae/diffusion_pytorch_model.safetensors not found\n",
      "{'image_encoder'} was not found in config. Values will be initialized to default values.\n",
      "\n",
      "Loading pipeline components...:   0%|                     | 0/5 [00:00<?, ?it/s]\u001b[A{'latents_mean', 'latents_std'} was not found in config. Values will be initialized to default values.\n",
      "Loaded vae as AutoencoderKL from `vae` subfolder of segmind/tiny-sd.\n",
      "\n",
      "Loading pipeline components...:  20%|‚ñà‚ñà‚ñå          | 1/5 [00:00<00:01,  2.87it/s]\u001b[A{'euler_at_final', 'final_sigmas_type', 'rescale_betas_zero_snr', 'use_lu_lambdas'} was not found in config. Values will be initialized to default values.\n",
      "Loaded scheduler as DPMSolverMultistepScheduler from `scheduler` subfolder of segmind/tiny-sd.\n",
      "Loaded tokenizer as CLIPTokenizer from `tokenizer` subfolder of segmind/tiny-sd.\n",
      "Loaded text_encoder as CLIPTextModel from `text_encoder` subfolder of segmind/tiny-sd.\n",
      "\n",
      "Loading pipeline components...: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5/5 [00:00<00:00,  6.72it/s]\u001b[A\n",
      "Steps:  12%| | 1760/15000 [1:20:24<8:49:17,  2.40s/it, lr=9.66e-5, step_loss=0.004/07/2024 18:53:14 - INFO - __main__ - Running validation... \n",
      " Generating 4 images with prompt: SUPER_PROMPT a nude woman on the beach.\n",
      "vae/diffusion_pytorch_model.safetensors not found\n",
      "{'image_encoder'} was not found in config. Values will be initialized to default values.\n",
      "\n",
      "Loading pipeline components...:   0%|                     | 0/5 [00:00<?, ?it/s]\u001b[A{'latents_mean', 'latents_std'} was not found in config. Values will be initialized to default values.\n",
      "Loaded vae as AutoencoderKL from `vae` subfolder of segmind/tiny-sd.\n",
      "\n",
      "Loading pipeline components...:  20%|‚ñà‚ñà‚ñå          | 1/5 [00:00<00:00,  4.34it/s]\u001b[A{'euler_at_final', 'final_sigmas_type', 'rescale_betas_zero_snr', 'use_lu_lambdas'} was not found in config. Values will be initialized to default values.\n",
      "Loaded scheduler as DPMSolverMultistepScheduler from `scheduler` subfolder of segmind/tiny-sd.\n",
      "Loaded tokenizer as CLIPTokenizer from `tokenizer` subfolder of segmind/tiny-sd.\n",
      "Loaded text_encoder as CLIPTextModel from `text_encoder` subfolder of segmind/tiny-sd.\n",
      "\n",
      "Loading pipeline components...: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5/5 [00:00<00:00,  8.01it/s]\u001b[A\n",
      "Steps:  12%| | 1870/15000 [1:25:47<8:46:30,  2.41s/it, lr=9.62e-5, step_loss=0.204/07/2024 18:58:37 - INFO - __main__ - Running validation... \n",
      " Generating 4 images with prompt: SUPER_PROMPT a nude woman on the beach.\n",
      "vae/diffusion_pytorch_model.safetensors not found\n",
      "{'image_encoder'} was not found in config. Values will be initialized to default values.\n",
      "\n",
      "Loading pipeline components...:   0%|                     | 0/5 [00:00<?, ?it/s]\u001b[A{'latents_mean', 'latents_std'} was not found in config. Values will be initialized to default values.\n",
      "Loaded vae as AutoencoderKL from `vae` subfolder of segmind/tiny-sd.\n",
      "\n",
      "Loading pipeline components...:  20%|‚ñà‚ñà‚ñå          | 1/5 [00:00<00:01,  3.80it/s]\u001b[A{'euler_at_final', 'final_sigmas_type', 'rescale_betas_zero_snr', 'use_lu_lambdas'} was not found in config. Values will be initialized to default values.\n",
      "Loaded scheduler as DPMSolverMultistepScheduler from `scheduler` subfolder of segmind/tiny-sd.\n",
      "Loaded tokenizer as CLIPTokenizer from `tokenizer` subfolder of segmind/tiny-sd.\n",
      "Loaded text_encoder as CLIPTextModel from `text_encoder` subfolder of segmind/tiny-sd.\n",
      "\n",
      "Loading pipeline components...: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5/5 [00:00<00:00,  7.63it/s]\u001b[A\n",
      "Steps:  13%|‚ñè| 1980/15000 [1:31:00<7:20:36,  2.03s/it, lr=9.58e-5, step_loss=0.004/07/2024 19:03:50 - INFO - __main__ - Running validation... \n",
      " Generating 4 images with prompt: SUPER_PROMPT a nude woman on the beach.\n",
      "vae/diffusion_pytorch_model.safetensors not found\n",
      "{'image_encoder'} was not found in config. Values will be initialized to default values.\n",
      "\n",
      "Loading pipeline components...:   0%|                     | 0/5 [00:00<?, ?it/s]\u001b[A{'latents_mean', 'latents_std'} was not found in config. Values will be initialized to default values.\n",
      "Loaded vae as AutoencoderKL from `vae` subfolder of segmind/tiny-sd.\n",
      "\n",
      "Loading pipeline components...:  20%|‚ñà‚ñà‚ñå          | 1/5 [00:00<00:01,  3.26it/s]\u001b[A{'euler_at_final', 'final_sigmas_type', 'rescale_betas_zero_snr', 'use_lu_lambdas'} was not found in config. Values will be initialized to default values.\n",
      "Loaded scheduler as DPMSolverMultistepScheduler from `scheduler` subfolder of segmind/tiny-sd.\n",
      "Loaded tokenizer as CLIPTokenizer from `tokenizer` subfolder of segmind/tiny-sd.\n",
      "Loaded text_encoder as CLIPTextModel from `text_encoder` subfolder of segmind/tiny-sd.\n",
      "\n",
      "Loading pipeline components...: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5/5 [00:00<00:00,  7.23it/s]\u001b[A\n",
      "Steps:  13%|‚ñè| 1986/15000 [1:31:29<11:03:28,  3.06s/it, lr=9.57e-5, step_loss=0."
     ]
    }
   ],
   "source": [
    "!accelerate launch train_text_to_image_lora.py \\\n",
    "  --pretrained_model_name_or_path=\"segmind/tiny-sd\" \\\n",
    "  --dataset_name=\"philipp-zettl/nsfw-images\" \\\n",
    "  --dataloader_num_workers=1 \\\n",
    "  --resolution=512 \\\n",
    "  --center_crop \\\n",
    "  --random_flip \\\n",
    "  --train_batch_size=1 \\\n",
    "  --gradient_accumulation_steps=4 \\\n",
    "  --max_train_steps=15000 \\\n",
    "  --learning_rate=1e-04 \\\n",
    "  --max_grad_norm=1 \\\n",
    "  --lr_scheduler=\"cosine\" \\\n",
    "  --lr_warmup_steps=0 \\\n",
    "  --output_dir=\"./finetune/lora/nsfw\" \\\n",
    "  --push_to_hub \\\n",
    "  --hub_model_id=\"philipp-zettl/tiny-sd-nsfw-lora\" \\\n",
    "  --report_to=wandb \\\n",
    "  --checkpointing_steps=500 \\\n",
    "  --validation_prompt=\"SUPER_PROMPT a nude woman on the beach\" \\\n",
    "  --seed=1337"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
